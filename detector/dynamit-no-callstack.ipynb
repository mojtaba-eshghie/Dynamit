{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# installing packages (if they do not exist)\n",
    "!conda install --yes numpy=1.18.5\n",
    "!conda install --yes pandas=1.2.0\n",
    "!conda install --yes scikit-learn=0.23.2\n",
    "!conda install --yes matplotlib=3.2.2\n",
    "!conda install --yes seaborn=0.10.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import string\n",
    "from sklearn.model_selection import KFold\n",
    "from decimal import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import svm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions For Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions may be used in case we need to do a data transformation into categorical data.\n",
    "\n",
    "def gas_bining(org_df):\n",
    "    df = org_df.copy()\n",
    "    \n",
    "    df['gas_used'] = pd.cut(df['gas_used'], bins=[i * 100000 for i in range(0, 20)], labels=list(string.ascii_uppercase[:19]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_balance_delta(org_df):\n",
    "    df = org_df.copy()\n",
    "\n",
    "\n",
    "    for i in range(0, df.shape[0]):\n",
    "\n",
    "        if int(df.at[i, 'victim_balance_delta']) == 0:\n",
    "            df.at[i, 'victim_balance_delta'] = 'zero'\n",
    "\n",
    "        elif int(df.at[i, 'victim_balance_delta']) > 0:\n",
    "            df.at[i, 'victim_balance_delta'] = 'positive'\n",
    "\n",
    "        elif int(df.at[i, 'victim_balance_delta']) < 0:\n",
    "            df.at[i, 'victim_balance_delta'] = 'negative'\n",
    "\n",
    "\n",
    "        if int(df.at[i, 'attacker_balance_delta']) == 0:\n",
    "            df.at[i, 'attacker_balance_delta'] = 'zero'\n",
    "\n",
    "        elif int(df.at[i, 'attacker_balance_delta']) > 0:\n",
    "            df.at[i, 'attacker_balance_delta'] = 'positive'\n",
    "\n",
    "        elif int(df.at[i, 'attacker_balance_delta']) < 0:\n",
    "            df.at[i, 'attacker_balance_delta'] = 'negative'\n",
    "\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def call_stack_depth_bining(org_df):\n",
    "    df = org_df.copy()\n",
    "\n",
    "    df['call_stack_depth'] = pd.cut(df['call_stack_depth'], bins=[i for i in range(0, 20)], labels=list(string.ascii_uppercase[:19]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_labels_col(org_df):\n",
    "    df = org_df.copy()\n",
    "    harmful = list()\n",
    "    labels = list(df['label'])\n",
    "    \n",
    "    for label in labels:\n",
    "        if label == 'safe':\n",
    "            harmful.append(False)\n",
    "        elif label == 'vul':\n",
    "            harmful.append(True)\n",
    "    \n",
    "    return harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def get_ys_from_prediction(prediction_df):\n",
    "    df = prediction_df.copy()\n",
    "    ys = list()\n",
    "\n",
    "    col_list = df.columns\n",
    "    for index, row in df.iterrows():\n",
    "        max_prob = row.max()\n",
    "        for col in col_list:\n",
    "            if row[col] == max_prob:\n",
    "                predicted = col\n",
    "                break\n",
    "        ys.append(predicted)\n",
    "    \n",
    "    return ys\n",
    "\n",
    "\n",
    "\n",
    "def get_fn_fp(y_pred, test_labels):\n",
    "    fp, fn, tp, tn = 0, 0, 0, 0\n",
    "    y_pred_list = list(y_pred)\n",
    "    for i in range(0, len(y_pred_list)):\n",
    "\n",
    "        if y_pred_list[i] != test_labels[i]:\n",
    "            if (y_pred_list[i] == \"vul\" and test_labels[i] == \"safe\"):\n",
    "                fp += 1\n",
    "            if (y_pred_list[i] == 'safe' and test_labels[i] == 'vul'):\n",
    "                fn += 1\n",
    "        else:\n",
    "            \n",
    "            if (y_pred_list[i] == 'vul' and test_labels[i] == 'vul'):\n",
    "                tp += 1\n",
    "            if (y_pred_list[i] == 'safe' and test_labels[i] == 'safe'):\n",
    "                tn += 1\n",
    "                \n",
    "    fnr = fn / (fn + tp)\n",
    "    fpr = fp / (fp + tn)\n",
    "\n",
    "    return (fnr, fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a constant list of random seeds for numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seeds = [31403, 31429, 31579, 31523, 31679, 31693, 31589, 31697, 32099, 31871, 31683, 32063, 31403, 32365, 32313, 32363, 32811, 31794, 32195, 31612, 32083, 32579, 32679, 32990, 33347, 32128, 32703, 32132, 31459, 32911, 32003, 33821, 33803, 33086, 34633, 33363, 32735, 31884, 33797, 33548, 34683, 34314, 31907, 31747, 34703, 35318, 31863, 31638, 33707, 33559, 35103, 35891, 35615, 35590, 36101, 33713, 35547, 35393, 31693, 33409, 33743, 32074, 32767, 35183, 35499, 37773, 34175, 38036, 35211, 32783, 35183, 33533, 36659, 37170, 33623, 35153, 37939, 34329, 34835, 32588, 34683, 35453, 32469, 38624, 35771, 36928, 32693, 34883, 34747, 37633, 34913, 36681, 32507, 33542, 37137, 34823, 34187, 34895, 41007, 40016]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our implementation of a Random Forest with Bagging based on sikitlearn\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.column_filter = None\n",
    "        self.imputation = None \n",
    "        self.one_hot = None \n",
    "        self.labels = None \n",
    "        self.model = None\n",
    "        \n",
    "\n",
    "        # HELPER\n",
    "        self.no_trees = None\n",
    "\n",
    "        # DEBUG\n",
    "        self.tree_predictions_list = None\n",
    "\n",
    "    \n",
    "    def fit(self, df, no_trees=100):\n",
    "        self.no_trees = no_trees\n",
    "        df = df.copy()\n",
    "\n",
    "        # initializing our random forest as a list of all our generated trees:\n",
    "        self.model = list()\n",
    "        \n",
    "        y = df['label'].values\n",
    "        df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "        \n",
    "        #df, self.one_hot = create_one_hot(df)\n",
    "        #df, self.column_filter = create_column_filter(df)\n",
    "        #df, self.imputation = create_imputation(df)\n",
    "        \n",
    "        x = df.values\n",
    "\n",
    "        # total number of features:\n",
    "        F_size = len(df.columns)\n",
    "\n",
    "\n",
    "        # just to use later!\n",
    "        df_with_classes = df.copy()\n",
    "        df_with_classes['label'] = pd.Series(y, index=df_with_classes.index)\n",
    "\n",
    "        def select_with_replacement(trees_i):\n",
    "            '''\n",
    "            This function will return a newly selected with replacement sample.\n",
    "            '''\n",
    "            np.random.seed(random_seeds[trees_i])\n",
    "            sample = pd.DataFrame(columns = df_with_classes.columns)\n",
    "            \n",
    "            np.random.seed(random_seeds[trees_i])\n",
    "            selections = np.random.choice(df_with_classes.shape[0], df_with_classes.shape[0], replace=True)\n",
    "            \n",
    "            for selection in selections:\n",
    "                sample = sample.append(df_with_classes.iloc[selection])\n",
    "                \n",
    "\n",
    "            return sample \n",
    "\n",
    "\n",
    "\n",
    "        for trees_i in range(0, no_trees):\n",
    "            \n",
    "            sample_df = select_with_replacement(trees_i)\n",
    "            \n",
    "            y = sample_df['label'].values\n",
    "            sample_df.drop(columns=['label'], inplace=True)\n",
    "            x = sample_df.values\n",
    "\n",
    "            self.model.append(DecisionTreeClassifier(max_features='log2'))\n",
    "            self.model[-1].fit(x, y)\n",
    "\n",
    "            print('tree # {} is built,'.format(trees_i))\n",
    "\n",
    "\n",
    "            # I used these lines to get insight into how are my decision trees doing (commented)\n",
    "            '''\n",
    "            tree_desc = tree.export_graphviz(self.model[-1], out_file='1_forest/{}.dot'.format(trees_i), feature_names=list(sample_df.columns))\n",
    "            dot_data = tree.export_graphviz(self.model[-1], feature_names=list(sample_df.columns), class_names=[str(c) for c in self.model[-1].classes_], filled=True, rounded=True)\n",
    "            graph = graphviz.Source(dot_data)\n",
    "            graph.render('1_forest/{}.gv'.format(trees_i), view=False)  \n",
    "            '''\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def predict(self, df):\n",
    "        test_df = df.copy()\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        #test_df = apply_one_hot(test_df, self.one_hot)\n",
    "        #test_df = apply_column_filter(test_df, self.column_filter)\n",
    "        #test_df = apply_imputation(test_df, self.imputation)\n",
    "\n",
    "        test_x = test_df.values\n",
    "\n",
    "        # let's add all predictions from all trees to a single list so that we can use it later!\n",
    "        # each index in this list denotes the prediction of a tree\n",
    "        tree_predictions_list = list()\n",
    "\n",
    "        for dt in self.model:\n",
    "            tree_predictions = dt.predict_proba(test_x)\n",
    "            tree_predictions_list.append(tree_predictions)\n",
    "\n",
    "        self.tree_predictions_list = tree_predictions_list\n",
    "\n",
    "\n",
    "\n",
    "        predictions = pd.DataFrame(columns=['safe' ,'vul'])\n",
    "\n",
    "        # averaging for each instance:\n",
    "        for instance_index in range(0, len(test_x)):\n",
    "            \n",
    "            total_negative = 0\n",
    "            total_positive = 0\n",
    "            \n",
    "            for model_index in range(0, self.no_trees):\n",
    "                total_negative += tree_predictions_list[model_index][instance_index][0]\n",
    "                total_positive += tree_predictions_list[model_index][instance_index][1]\n",
    "            \n",
    "            \n",
    "\n",
    "            predictions = predictions.append({\n",
    "                'safe': Decimal(total_negative) / Decimal(self.no_trees),\n",
    "                'vul': Decimal(total_positive) / Decimal(self.no_trees)\n",
    "            }, ignore_index=True)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(original_df, correctlabels):\n",
    "    \n",
    "    df = original_df.copy()\n",
    "    \n",
    "    '''\n",
    "    Assumption: I assume that the accuracy is ratio `number of data instances \n",
    "    correctly classified / total no of data instances`\n",
    "    Furthermore, for an instance that does not have a label with highest probability,\n",
    "    I will choose the first label in column orders as the predicted label for that instance.\n",
    "    '''\n",
    "    \n",
    "    cor_pred = list() # correct predictions list\n",
    "    inc_pred = list() # incorrect predictions list\n",
    "    col_list = df.columns\n",
    "    for index, row in df.iterrows():\n",
    "        max_prob = row.max()\n",
    "        for col in col_list:\n",
    "            if row[col] == max_prob:\n",
    "                predicted = col\n",
    "                break\n",
    "        correct = correctlabels[index]\n",
    "        if correct == predicted:\n",
    "            cor_pred.append(index)\n",
    "        else:\n",
    "            inc_pred.append(index)\n",
    "    \n",
    "    return len(cor_pred)/ len(correctlabels)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('data/transactions2.csv')\n",
    "\n",
    "df.drop(columns=['call_stack_depth'], inplace=True)\n",
    "\n",
    "\n",
    "# The following dictionary will be used to hold the performance measurements of our classifiers\n",
    "perfs_dict = {\n",
    "    'nb': {\n",
    "    },\n",
    "    'lr': {\n",
    "    }, \n",
    "    'knn': {\n",
    "    }, \n",
    "    'svm': {\n",
    "    },\n",
    "    'rf': {\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [  0   3   5   8   9  10  11  12  13  14  15  16  17  18  19  20  21  23\n",
      "  24  25  26  28  29  30  31  32  33  34  35  37  38  39  40  41  42  43\n",
      "  44  45  46  47  48  49  50  51  52  55  56  57  58  59  60  61  62  64\n",
      "  67  68  70  71  74  75  76  77  78  79  80  81  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  99 100 101 102 103 104] \n",
      "TEST: [ 1  2  4  6  7 22 27 36 53 54 63 65 66 69 72 73 82 83 84 97 98]\n",
      "tree # 0 is built,\n",
      "tree # 1 is built,\n",
      "tree # 2 is built,\n",
      "tree # 3 is built,\n",
      "tree # 4 is built,\n",
      "tree # 5 is built,\n",
      "tree # 6 is built,\n",
      "tree # 7 is built,\n",
      "tree # 8 is built,\n",
      "tree # 9 is built,\n",
      "tree # 10 is built,\n",
      "tree # 11 is built,\n",
      "tree # 12 is built,\n",
      "tree # 13 is built,\n",
      "tree # 14 is built,\n",
      "tree # 15 is built,\n",
      "tree # 16 is built,\n",
      "tree # 17 is built,\n",
      "tree # 18 is built,\n",
      "tree # 19 is built,\n",
      "tree # 20 is built,\n",
      "tree # 21 is built,\n",
      "tree # 22 is built,\n",
      "tree # 23 is built,\n",
      "tree # 24 is built,\n",
      "tree # 25 is built,\n",
      "tree # 26 is built,\n",
      "tree # 27 is built,\n",
      "tree # 28 is built,\n",
      "tree # 29 is built,\n",
      "tree # 30 is built,\n",
      "tree # 31 is built,\n",
      "tree # 32 is built,\n",
      "tree # 33 is built,\n",
      "tree # 34 is built,\n",
      "tree # 35 is built,\n",
      "tree # 36 is built,\n",
      "tree # 37 is built,\n",
      "tree # 38 is built,\n",
      "tree # 39 is built,\n",
      "tree # 40 is built,\n",
      "tree # 41 is built,\n",
      "tree # 42 is built,\n",
      "tree # 43 is built,\n",
      "tree # 44 is built,\n",
      "tree # 45 is built,\n",
      "tree # 46 is built,\n",
      "tree # 47 is built,\n",
      "tree # 48 is built,\n",
      "tree # 49 is built,\n",
      "tree # 50 is built,\n",
      "tree # 51 is built,\n",
      "tree # 52 is built,\n",
      "tree # 53 is built,\n",
      "tree # 54 is built,\n",
      "tree # 55 is built,\n",
      "tree # 56 is built,\n",
      "tree # 57 is built,\n",
      "tree # 58 is built,\n",
      "tree # 59 is built,\n",
      "tree # 60 is built,\n",
      "tree # 61 is built,\n",
      "tree # 62 is built,\n",
      "tree # 63 is built,\n",
      "tree # 64 is built,\n",
      "tree # 65 is built,\n",
      "tree # 66 is built,\n",
      "tree # 67 is built,\n",
      "tree # 68 is built,\n",
      "tree # 69 is built,\n",
      "tree # 70 is built,\n",
      "tree # 71 is built,\n",
      "tree # 72 is built,\n",
      "tree # 73 is built,\n",
      "tree # 74 is built,\n",
      "tree # 75 is built,\n",
      "tree # 76 is built,\n",
      "tree # 77 is built,\n",
      "tree # 78 is built,\n",
      "tree # 79 is built,\n",
      "tree # 80 is built,\n",
      "tree # 81 is built,\n",
      "tree # 82 is built,\n",
      "tree # 83 is built,\n",
      "tree # 84 is built,\n",
      "tree # 85 is built,\n",
      "tree # 86 is built,\n",
      "tree # 87 is built,\n",
      "tree # 88 is built,\n",
      "tree # 89 is built,\n",
      "tree # 90 is built,\n",
      "tree # 91 is built,\n",
      "tree # 92 is built,\n",
      "tree # 93 is built,\n",
      "tree # 94 is built,\n",
      "tree # 95 is built,\n",
      "tree # 96 is built,\n",
      "tree # 97 is built,\n",
      "tree # 98 is built,\n",
      "tree # 99 is built,\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       1.00      0.90      0.95        10\n",
      "         vul       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.95        21\n",
      "   macro avg       0.96      0.95      0.95        21\n",
      "weighted avg       0.96      0.95      0.95        21\n",
      "\n",
      "fn is: 0.0, fp is: 0.1\n",
      "\n",
      "Accuracy of prediction is: 0.9523809523809523\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   4   5   6   7   8   9  13  14  15  16  18  19  20  21  22\n",
      "  24  25  26  27  28  29  30  32  33  34  35  36  38  40  41  43  44  45\n",
      "  46  47  50  51  52  53  54  55  56  57  58  59  60  61  62  63  65  66\n",
      "  67  68  69  70  72  73  75  76  77  78  80  82  83  84  85  86  87  88\n",
      "  89  90  91  92  93  95  96  97  98  99 100 101] \n",
      "TEST: [  3  10  11  12  17  23  31  37  39  42  48  49  64  71  74  79  81  94\n",
      " 102 103 104]\n",
      "tree # 0 is built,\n",
      "tree # 1 is built,\n",
      "tree # 2 is built,\n",
      "tree # 3 is built,\n",
      "tree # 4 is built,\n",
      "tree # 5 is built,\n",
      "tree # 6 is built,\n",
      "tree # 7 is built,\n",
      "tree # 8 is built,\n",
      "tree # 9 is built,\n",
      "tree # 10 is built,\n",
      "tree # 11 is built,\n",
      "tree # 12 is built,\n",
      "tree # 13 is built,\n",
      "tree # 14 is built,\n",
      "tree # 15 is built,\n",
      "tree # 16 is built,\n",
      "tree # 17 is built,\n",
      "tree # 18 is built,\n",
      "tree # 19 is built,\n",
      "tree # 20 is built,\n",
      "tree # 21 is built,\n",
      "tree # 22 is built,\n",
      "tree # 23 is built,\n",
      "tree # 24 is built,\n",
      "tree # 25 is built,\n",
      "tree # 26 is built,\n",
      "tree # 27 is built,\n",
      "tree # 28 is built,\n",
      "tree # 29 is built,\n",
      "tree # 30 is built,\n",
      "tree # 31 is built,\n",
      "tree # 32 is built,\n",
      "tree # 33 is built,\n",
      "tree # 34 is built,\n",
      "tree # 35 is built,\n",
      "tree # 36 is built,\n",
      "tree # 37 is built,\n",
      "tree # 38 is built,\n",
      "tree # 39 is built,\n",
      "tree # 40 is built,\n",
      "tree # 41 is built,\n",
      "tree # 42 is built,\n",
      "tree # 43 is built,\n",
      "tree # 44 is built,\n",
      "tree # 45 is built,\n",
      "tree # 46 is built,\n",
      "tree # 47 is built,\n",
      "tree # 48 is built,\n",
      "tree # 49 is built,\n",
      "tree # 50 is built,\n",
      "tree # 51 is built,\n",
      "tree # 52 is built,\n",
      "tree # 53 is built,\n",
      "tree # 54 is built,\n",
      "tree # 55 is built,\n",
      "tree # 56 is built,\n",
      "tree # 57 is built,\n",
      "tree # 58 is built,\n",
      "tree # 59 is built,\n",
      "tree # 60 is built,\n",
      "tree # 61 is built,\n",
      "tree # 62 is built,\n",
      "tree # 63 is built,\n",
      "tree # 64 is built,\n",
      "tree # 65 is built,\n",
      "tree # 66 is built,\n",
      "tree # 67 is built,\n",
      "tree # 68 is built,\n",
      "tree # 69 is built,\n",
      "tree # 70 is built,\n",
      "tree # 71 is built,\n",
      "tree # 72 is built,\n",
      "tree # 73 is built,\n",
      "tree # 74 is built,\n",
      "tree # 75 is built,\n",
      "tree # 76 is built,\n",
      "tree # 77 is built,\n",
      "tree # 78 is built,\n",
      "tree # 79 is built,\n",
      "tree # 80 is built,\n",
      "tree # 81 is built,\n",
      "tree # 82 is built,\n",
      "tree # 83 is built,\n",
      "tree # 84 is built,\n",
      "tree # 85 is built,\n",
      "tree # 86 is built,\n",
      "tree # 87 is built,\n",
      "tree # 88 is built,\n",
      "tree # 89 is built,\n",
      "tree # 90 is built,\n",
      "tree # 91 is built,\n",
      "tree # 92 is built,\n",
      "tree # 93 is built,\n",
      "tree # 94 is built,\n",
      "tree # 95 is built,\n",
      "tree # 96 is built,\n",
      "tree # 97 is built,\n",
      "tree # 98 is built,\n",
      "tree # 99 is built,\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.91      0.91      0.91        11\n",
      "         vul       0.90      0.90      0.90        10\n",
      "\n",
      "    accuracy                           0.90        21\n",
      "   macro avg       0.90      0.90      0.90        21\n",
      "weighted avg       0.90      0.90      0.90        21\n",
      "\n",
      "fn is: 0.1, fp is: 0.09090909090909091\n",
      "\n",
      "Accuracy of prediction is: 0.9047619047619048\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  16  17  18  19  21  22\n",
      "  23  25  26  27  28  29  31  33  34  36  37  39  40  41  42  43  45  46\n",
      "  47  48  49  52  53  54  55  56  57  59  60  61  62  63  64  65  66  67\n",
      "  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86\n",
      "  87  90  91  92  94  97  98  99 100 102 103 104] \n",
      "TEST: [  5  13  14  15  20  24  30  32  35  38  44  50  51  58  68  88  89  93\n",
      "  95  96 101]\n",
      "tree # 0 is built,\n",
      "tree # 1 is built,\n",
      "tree # 2 is built,\n",
      "tree # 3 is built,\n",
      "tree # 4 is built,\n",
      "tree # 5 is built,\n",
      "tree # 6 is built,\n",
      "tree # 7 is built,\n",
      "tree # 8 is built,\n",
      "tree # 9 is built,\n",
      "tree # 10 is built,\n",
      "tree # 11 is built,\n",
      "tree # 12 is built,\n",
      "tree # 13 is built,\n",
      "tree # 14 is built,\n",
      "tree # 15 is built,\n",
      "tree # 16 is built,\n",
      "tree # 17 is built,\n",
      "tree # 18 is built,\n",
      "tree # 19 is built,\n",
      "tree # 20 is built,\n",
      "tree # 21 is built,\n",
      "tree # 22 is built,\n",
      "tree # 23 is built,\n",
      "tree # 24 is built,\n",
      "tree # 25 is built,\n",
      "tree # 26 is built,\n",
      "tree # 27 is built,\n",
      "tree # 28 is built,\n",
      "tree # 29 is built,\n",
      "tree # 30 is built,\n",
      "tree # 31 is built,\n",
      "tree # 32 is built,\n",
      "tree # 33 is built,\n",
      "tree # 34 is built,\n",
      "tree # 35 is built,\n",
      "tree # 36 is built,\n",
      "tree # 37 is built,\n",
      "tree # 38 is built,\n",
      "tree # 39 is built,\n",
      "tree # 40 is built,\n",
      "tree # 41 is built,\n",
      "tree # 42 is built,\n",
      "tree # 43 is built,\n",
      "tree # 44 is built,\n",
      "tree # 45 is built,\n",
      "tree # 46 is built,\n",
      "tree # 47 is built,\n",
      "tree # 48 is built,\n",
      "tree # 49 is built,\n",
      "tree # 50 is built,\n",
      "tree # 51 is built,\n",
      "tree # 52 is built,\n",
      "tree # 53 is built,\n",
      "tree # 54 is built,\n",
      "tree # 55 is built,\n",
      "tree # 56 is built,\n",
      "tree # 57 is built,\n",
      "tree # 58 is built,\n",
      "tree # 59 is built,\n",
      "tree # 60 is built,\n",
      "tree # 61 is built,\n",
      "tree # 62 is built,\n",
      "tree # 63 is built,\n",
      "tree # 64 is built,\n",
      "tree # 65 is built,\n",
      "tree # 66 is built,\n",
      "tree # 67 is built,\n",
      "tree # 68 is built,\n",
      "tree # 69 is built,\n",
      "tree # 70 is built,\n",
      "tree # 71 is built,\n",
      "tree # 72 is built,\n",
      "tree # 73 is built,\n",
      "tree # 74 is built,\n",
      "tree # 75 is built,\n",
      "tree # 76 is built,\n",
      "tree # 77 is built,\n",
      "tree # 78 is built,\n",
      "tree # 79 is built,\n",
      "tree # 80 is built,\n",
      "tree # 81 is built,\n",
      "tree # 82 is built,\n",
      "tree # 83 is built,\n",
      "tree # 84 is built,\n",
      "tree # 85 is built,\n",
      "tree # 86 is built,\n",
      "tree # 87 is built,\n",
      "tree # 88 is built,\n",
      "tree # 89 is built,\n",
      "tree # 90 is built,\n",
      "tree # 91 is built,\n",
      "tree # 92 is built,\n",
      "tree # 93 is built,\n",
      "tree # 94 is built,\n",
      "tree # 95 is built,\n",
      "tree # 96 is built,\n",
      "tree # 97 is built,\n",
      "tree # 98 is built,\n",
      "tree # 99 is built,\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.90      1.00      0.95         9\n",
      "         vul       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.95        21\n",
      "   macro avg       0.95      0.96      0.95        21\n",
      "weighted avg       0.96      0.95      0.95        21\n",
      "\n",
      "fn is: 0.08333333333333333, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.9523809523809523\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  17  18  19\n",
      "  20  21  22  23  24  27  29  30  31  32  34  35  36  37  38  39  40  41\n",
      "  42  44  45  48  49  50  51  52  53  54  57  58  60  63  64  65  66  67\n",
      "  68  69  70  71  72  73  74  76  77  79  81  82  83  84  86  88  89  90\n",
      "  92  93  94  95  96  97  98 100 101 102 103 104] \n",
      "TEST: [ 8 16 25 26 28 33 43 46 47 55 56 59 61 62 75 78 80 85 87 91 99]\n",
      "tree # 0 is built,\n",
      "tree # 1 is built,\n",
      "tree # 2 is built,\n",
      "tree # 3 is built,\n",
      "tree # 4 is built,\n",
      "tree # 5 is built,\n",
      "tree # 6 is built,\n",
      "tree # 7 is built,\n",
      "tree # 8 is built,\n",
      "tree # 9 is built,\n",
      "tree # 10 is built,\n",
      "tree # 11 is built,\n",
      "tree # 12 is built,\n",
      "tree # 13 is built,\n",
      "tree # 14 is built,\n",
      "tree # 15 is built,\n",
      "tree # 16 is built,\n",
      "tree # 17 is built,\n",
      "tree # 18 is built,\n",
      "tree # 19 is built,\n",
      "tree # 20 is built,\n",
      "tree # 21 is built,\n",
      "tree # 22 is built,\n",
      "tree # 23 is built,\n",
      "tree # 24 is built,\n",
      "tree # 25 is built,\n",
      "tree # 26 is built,\n",
      "tree # 27 is built,\n",
      "tree # 28 is built,\n",
      "tree # 29 is built,\n",
      "tree # 30 is built,\n",
      "tree # 31 is built,\n",
      "tree # 32 is built,\n",
      "tree # 33 is built,\n",
      "tree # 34 is built,\n",
      "tree # 35 is built,\n",
      "tree # 36 is built,\n",
      "tree # 37 is built,\n",
      "tree # 38 is built,\n",
      "tree # 39 is built,\n",
      "tree # 40 is built,\n",
      "tree # 41 is built,\n",
      "tree # 42 is built,\n",
      "tree # 43 is built,\n",
      "tree # 44 is built,\n",
      "tree # 45 is built,\n",
      "tree # 46 is built,\n",
      "tree # 47 is built,\n",
      "tree # 48 is built,\n",
      "tree # 49 is built,\n",
      "tree # 50 is built,\n",
      "tree # 51 is built,\n",
      "tree # 52 is built,\n",
      "tree # 53 is built,\n",
      "tree # 54 is built,\n",
      "tree # 55 is built,\n",
      "tree # 56 is built,\n",
      "tree # 57 is built,\n",
      "tree # 58 is built,\n",
      "tree # 59 is built,\n",
      "tree # 60 is built,\n",
      "tree # 61 is built,\n",
      "tree # 62 is built,\n",
      "tree # 63 is built,\n",
      "tree # 64 is built,\n",
      "tree # 65 is built,\n",
      "tree # 66 is built,\n",
      "tree # 67 is built,\n",
      "tree # 68 is built,\n",
      "tree # 69 is built,\n",
      "tree # 70 is built,\n",
      "tree # 71 is built,\n",
      "tree # 72 is built,\n",
      "tree # 73 is built,\n",
      "tree # 74 is built,\n",
      "tree # 75 is built,\n",
      "tree # 76 is built,\n",
      "tree # 77 is built,\n",
      "tree # 78 is built,\n",
      "tree # 79 is built,\n",
      "tree # 80 is built,\n",
      "tree # 81 is built,\n",
      "tree # 82 is built,\n",
      "tree # 83 is built,\n",
      "tree # 84 is built,\n",
      "tree # 85 is built,\n",
      "tree # 86 is built,\n",
      "tree # 87 is built,\n",
      "tree # 88 is built,\n",
      "tree # 89 is built,\n",
      "tree # 90 is built,\n",
      "tree # 91 is built,\n",
      "tree # 92 is built,\n",
      "tree # 93 is built,\n",
      "tree # 94 is built,\n",
      "tree # 95 is built,\n",
      "tree # 96 is built,\n",
      "tree # 97 is built,\n",
      "tree # 98 is built,\n",
      "tree # 99 is built,\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       1.00      1.00      1.00        13\n",
      "         vul       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        21\n",
      "   macro avg       1.00      1.00      1.00        21\n",
      "weighted avg       1.00      1.00      1.00        21\n",
      "\n",
      "fn is: 0.0, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 1.0\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  20  22\n",
      "  23  24  25  26  27  28  30  31  32  33  35  36  37  38  39  42  43  44\n",
      "  46  47  48  49  50  51  53  54  55  56  58  59  61  62  63  64  65  66\n",
      "  68  69  71  72  73  74  75  78  79  80  81  82  83  84  85  87  88  89\n",
      "  91  93  94  95  96  97  98  99 101 102 103 104] \n",
      "TEST: [  0   9  18  19  21  29  34  40  41  45  52  57  60  67  70  76  77  86\n",
      "  90  92 100]\n",
      "tree # 0 is built,\n",
      "tree # 1 is built,\n",
      "tree # 2 is built,\n",
      "tree # 3 is built,\n",
      "tree # 4 is built,\n",
      "tree # 5 is built,\n",
      "tree # 6 is built,\n",
      "tree # 7 is built,\n",
      "tree # 8 is built,\n",
      "tree # 9 is built,\n",
      "tree # 10 is built,\n",
      "tree # 11 is built,\n",
      "tree # 12 is built,\n",
      "tree # 13 is built,\n",
      "tree # 14 is built,\n",
      "tree # 15 is built,\n",
      "tree # 16 is built,\n",
      "tree # 17 is built,\n",
      "tree # 18 is built,\n",
      "tree # 19 is built,\n",
      "tree # 20 is built,\n",
      "tree # 21 is built,\n",
      "tree # 22 is built,\n",
      "tree # 23 is built,\n",
      "tree # 24 is built,\n",
      "tree # 25 is built,\n",
      "tree # 26 is built,\n",
      "tree # 27 is built,\n",
      "tree # 28 is built,\n",
      "tree # 29 is built,\n",
      "tree # 30 is built,\n",
      "tree # 31 is built,\n",
      "tree # 32 is built,\n",
      "tree # 33 is built,\n",
      "tree # 34 is built,\n",
      "tree # 35 is built,\n",
      "tree # 36 is built,\n",
      "tree # 37 is built,\n",
      "tree # 38 is built,\n",
      "tree # 39 is built,\n",
      "tree # 40 is built,\n",
      "tree # 41 is built,\n",
      "tree # 42 is built,\n",
      "tree # 43 is built,\n",
      "tree # 44 is built,\n",
      "tree # 45 is built,\n",
      "tree # 46 is built,\n",
      "tree # 47 is built,\n",
      "tree # 48 is built,\n",
      "tree # 49 is built,\n",
      "tree # 50 is built,\n",
      "tree # 51 is built,\n",
      "tree # 52 is built,\n",
      "tree # 53 is built,\n",
      "tree # 54 is built,\n",
      "tree # 55 is built,\n",
      "tree # 56 is built,\n",
      "tree # 57 is built,\n",
      "tree # 58 is built,\n",
      "tree # 59 is built,\n",
      "tree # 60 is built,\n",
      "tree # 61 is built,\n",
      "tree # 62 is built,\n",
      "tree # 63 is built,\n",
      "tree # 64 is built,\n",
      "tree # 65 is built,\n",
      "tree # 66 is built,\n",
      "tree # 67 is built,\n",
      "tree # 68 is built,\n",
      "tree # 69 is built,\n",
      "tree # 70 is built,\n",
      "tree # 71 is built,\n",
      "tree # 72 is built,\n",
      "tree # 73 is built,\n",
      "tree # 74 is built,\n",
      "tree # 75 is built,\n",
      "tree # 76 is built,\n",
      "tree # 77 is built,\n",
      "tree # 78 is built,\n",
      "tree # 79 is built,\n",
      "tree # 80 is built,\n",
      "tree # 81 is built,\n",
      "tree # 82 is built,\n",
      "tree # 83 is built,\n",
      "tree # 84 is built,\n",
      "tree # 85 is built,\n",
      "tree # 86 is built,\n",
      "tree # 87 is built,\n",
      "tree # 88 is built,\n",
      "tree # 89 is built,\n",
      "tree # 90 is built,\n",
      "tree # 91 is built,\n",
      "tree # 92 is built,\n",
      "tree # 93 is built,\n",
      "tree # 94 is built,\n",
      "tree # 95 is built,\n",
      "tree # 96 is built,\n",
      "tree # 97 is built,\n",
      "tree # 98 is built,\n",
      "tree # 99 is built,\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.93      1.00      0.96        13\n",
      "         vul       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.95        21\n",
      "   macro avg       0.96      0.94      0.95        21\n",
      "weighted avg       0.96      0.95      0.95        21\n",
      "\n",
      "fn is: 0.125, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.9523809523809523\n",
      "========================================================\n",
      "========================================================\n",
      "*********************************************************\n",
      "\n",
      "Average accuracy: 0.9523809523809523\n",
      "---------------------------------------------------------\n",
      "{'nb': {}, 'lr': {}, 'knn': {}, 'svm': {}, 'rf': {'fp': 0.038181818181818185, 'fn': 0.06166666666666667, 'ac': 0.9523809523809523, 'f1': 0.9492753623188406, 'recall': 0.9383333333333332}}\n"
     ]
    }
   ],
   "source": [
    "# Random Forest \n",
    "\n",
    "# trying to scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "rf = RandomForest()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "labels = list(df['label'])\n",
    "\n",
    "accuracies = list()\n",
    "fn_list = list()\n",
    "fp_list = list()\n",
    "recall_list = list()\n",
    "f1_list = list()\n",
    "\n",
    "counter = 0 \n",
    "for train_indices, test_indices in kf.split(df):\n",
    "    \n",
    "    \n",
    "    #pd.DataFrame(train_indices).to_csv(\"data/processing_steps/train_indices_{}.csv\".format(counter))\n",
    "    #pd.DataFrame(test_indices).to_csv(\"data/processing_steps/test_indices_{}.csv\".format(counter))\n",
    "\n",
    "    train_indices = pd.read_csv(\"data/processing_steps/train_indices_{}.csv\".format(counter))\n",
    "    test_indices = pd.read_csv(\"data/processing_steps/test_indices_{}.csv\".format(counter))\n",
    "    \n",
    "    train_indices.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "    test_indices.drop(columns=[\"Unnamed: 0\"], inplace=True) \n",
    "\n",
    "    train_indices = train_indices['0'].to_numpy()\n",
    "    test_indices = test_indices['0'].to_numpy()\n",
    "\n",
    "    print(\"TRAIN:\", train_indices, \"\\nTEST:\", test_indices)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    train_df = df.loc[train_indices, :].copy()\n",
    "    test_df = df.loc[test_indices, :].copy()\n",
    "    \n",
    "    train_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    train_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    \n",
    "    test_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    test_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    test_labels = list(test_df['label'])\n",
    "    test_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    train_labels = list(train_df['label'])\n",
    "    train_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    # scaling:\n",
    "    scaler.fit(train_df)\n",
    "    train_ndarray = scaler.transform(train_df)\n",
    "    test_ndarray = scaler.transform(test_df)\n",
    "\n",
    "    # let's add the labels to the train_df after scalling, the fit function in our Random Forest classifier needs them:\n",
    "    train_df = pd.DataFrame(train_ndarray)\n",
    "    train_df['label'] = train_labels\n",
    "    test_df = pd.DataFrame(test_ndarray)\n",
    "    \n",
    "    rf.fit(train_df, no_trees=100)\n",
    "    predictions = rf.predict(test_df)\n",
    "\n",
    "    y_pred = get_ys_from_prediction(predictions)\n",
    "    print(classification_report(test_labels, y_pred))\n",
    "\n",
    "    #print(test_labels)\n",
    "    #print(y_pred)\n",
    "\n",
    "\n",
    "    # classification report (cr)\n",
    "    cr = classification_report(test_labels, y_pred, output_dict=True)\n",
    "    recall_list.append(cr['vul']['recall'])\n",
    "    f1_list.append(cr['vul']['f1-score'])\n",
    "\n",
    "    ac = metrics.accuracy_score(test_labels, y_pred)\n",
    "    accuracies.append(ac)\n",
    "\n",
    "\n",
    "    # obtain fp and fn\n",
    "    fn, fp = get_fn_fp(y_pred, test_labels)\n",
    "    print('fn is: {0}, fp is: {1}'.format(fn, fp))\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "\n",
    "    ac = metrics.accuracy_score(test_labels, y_pred)\n",
    "    #ac = accuracy(predictions, labels)\n",
    "    accuracies.append(ac)\n",
    "\n",
    "    \n",
    "    print(\"\\nAccuracy of prediction is: {}\".format(ac))\n",
    "    print(\"========================================================\")\n",
    "    print(\"========================================================\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"*********************************************************\")\n",
    "print(\"\\nAverage accuracy: {}\".format(sum(accuracies) / len(accuracies)))\n",
    "\n",
    "perfs_dict['rf']['fp'] = sum(fp_list) / len(fp_list)\n",
    "perfs_dict['rf']['fn'] = sum(fn_list) / len(fn_list)\n",
    "perfs_dict['rf']['ac'] = sum(accuracies) / len(accuracies)\n",
    "perfs_dict['rf']['f1'] = sum(f1_list) / len(f1_list)\n",
    "perfs_dict['rf']['recall'] = sum(recall_list) / len(recall_list)\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print(perfs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [  0   3   5   8   9  10  11  12  13  14  15  16  17  18  19  20  21  23\n",
      "  24  25  26  28  29  30  31  32  33  34  35  37  38  39  40  41  42  43\n",
      "  44  45  46  47  48  49  50  51  52  55  56  57  58  59  60  61  62  64\n",
      "  67  68  70  71  74  75  76  77  78  79  80  81  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  99 100 101 102 103 104] \n",
      "TEST: [ 1  2  4  6  7 22 27 36 53 54 63 65 66 69 72 73 82 83 84 97 98]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.77      1.00      0.87        10\n",
      "         vul       1.00      0.73      0.84        11\n",
      "\n",
      "    accuracy                           0.86        21\n",
      "   macro avg       0.88      0.86      0.86        21\n",
      "weighted avg       0.89      0.86      0.86        21\n",
      "\n",
      "fn is: 0.2727272727272727, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.8571428571428571\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   4   5   6   7   8   9  13  14  15  16  18  19  20  21  22\n",
      "  24  25  26  27  28  29  30  32  33  34  35  36  38  40  41  43  44  45\n",
      "  46  47  50  51  52  53  54  55  56  57  58  59  60  61  62  63  65  66\n",
      "  67  68  69  70  72  73  75  76  77  78  80  82  83  84  85  86  87  88\n",
      "  89  90  91  92  93  95  96  97  98  99 100 101] \n",
      "TEST: [  3  10  11  12  17  23  31  37  39  42  48  49  64  71  74  79  81  94\n",
      " 102 103 104]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.92      1.00      0.96        11\n",
      "         vul       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.95        21\n",
      "   macro avg       0.96      0.95      0.95        21\n",
      "weighted avg       0.96      0.95      0.95        21\n",
      "\n",
      "fn is: 0.1, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.9523809523809523\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  16  17  18  19  21  22\n",
      "  23  25  26  27  28  29  31  33  34  36  37  39  40  41  42  43  45  46\n",
      "  47  48  49  52  53  54  55  56  57  59  60  61  62  63  64  65  66  67\n",
      "  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86\n",
      "  87  90  91  92  94  97  98  99 100 102 103 104] \n",
      "TEST: [  5  13  14  15  20  24  30  32  35  38  44  50  51  58  68  88  89  93\n",
      "  95  96 101]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.64      1.00      0.78         9\n",
      "         vul       1.00      0.58      0.74        12\n",
      "\n",
      "    accuracy                           0.76        21\n",
      "   macro avg       0.82      0.79      0.76        21\n",
      "weighted avg       0.85      0.76      0.76        21\n",
      "\n",
      "fn is: 0.4166666666666667, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.7619047619047619\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  17  18  19\n",
      "  20  21  22  23  24  27  29  30  31  32  34  35  36  37  38  39  40  41\n",
      "  42  44  45  48  49  50  51  52  53  54  57  58  60  63  64  65  66  67\n",
      "  68  69  70  71  72  73  74  76  77  79  81  82  83  84  86  88  89  90\n",
      "  92  93  94  95  96  97  98 100 101 102 103 104] \n",
      "TEST: [ 8 16 25 26 28 33 43 46 47 55 56 59 61 62 75 78 80 85 87 91 99]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.93      1.00      0.96        13\n",
      "         vul       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.95        21\n",
      "   macro avg       0.96      0.94      0.95        21\n",
      "weighted avg       0.96      0.95      0.95        21\n",
      "\n",
      "fn is: 0.125, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.9523809523809523\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  20  22\n",
      "  23  24  25  26  27  28  30  31  32  33  35  36  37  38  39  42  43  44\n",
      "  46  47  48  49  50  51  53  54  55  56  58  59  61  62  63  64  65  66\n",
      "  68  69  71  72  73  74  75  78  79  80  81  82  83  84  85  87  88  89\n",
      "  91  93  94  95  96  97  98  99 101 102 103 104] \n",
      "TEST: [  0   9  18  19  21  29  34  40  41  45  52  57  60  67  70  76  77  86\n",
      "  90  92 100]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.81      1.00      0.90        13\n",
      "         vul       1.00      0.62      0.77         8\n",
      "\n",
      "    accuracy                           0.86        21\n",
      "   macro avg       0.91      0.81      0.83        21\n",
      "weighted avg       0.88      0.86      0.85        21\n",
      "\n",
      "fn is: 0.375, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.8571428571428571\n",
      "========================================================\n",
      "========================================================\n",
      "*********************************************************\n",
      "\n",
      "Average accuracy: 0.8761904761904761\n",
      "---------------------------------------------------------\n",
      "{'nb': {}, 'lr': {'fp': 0.0, 'fn': 0.2578787878787879, 'ac': 0.8761904761904761, 'f1': 0.8457759784075574, 'recall': 0.7421212121212122}, 'knn': {}, 'svm': {}, 'rf': {'fp': 0.038181818181818185, 'fn': 0.06166666666666667, 'ac': 0.9523809523809523, 'f1': 0.9492753623188406, 'recall': 0.9383333333333332}}\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# trying to scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "labels = list(df['label'])\n",
    "\n",
    "accuracies = list()\n",
    "fn_list = list()\n",
    "fp_list = list()\n",
    "recall_list = list()\n",
    "f1_list = list()\n",
    "\n",
    "counter = 0\n",
    "for train_indices, test_indices in kf.split(df):\n",
    "\n",
    "    train_indices = pd.read_csv(\"data/processing_steps/train_indices_{}.csv\".format(counter))\n",
    "    test_indices = pd.read_csv(\"data/processing_steps/test_indices_{}.csv\".format(counter))\n",
    "    \n",
    "    train_indices.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "    test_indices.drop(columns=[\"Unnamed: 0\"], inplace=True) \n",
    "\n",
    "    train_indices = train_indices['0'].to_numpy()\n",
    "    test_indices = test_indices['0'].to_numpy()\n",
    "\n",
    "    print(\"TRAIN:\", train_indices, \"\\nTEST:\", test_indices)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    \n",
    "    train_df = df.loc[train_indices, :].copy()\n",
    "    test_df = df.loc[test_indices, :].copy()\n",
    "    \n",
    "    train_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    train_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    \n",
    "    test_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    test_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    train_labels = list(train_df['label'])\n",
    "    train_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    test_labels = list(test_df['label'])\n",
    "    test_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    # scaling:\n",
    "    scaler.fit(train_df)\n",
    "    train_df = scaler.transform(train_df)\n",
    "    test_df = scaler.transform(test_df)\n",
    "\n",
    "    # training\n",
    "    logreg = LogisticRegression() \n",
    "    logreg.fit(train_df, train_labels)\n",
    "\n",
    "    # prediction\n",
    "    y_pred=logreg.predict(test_df)\n",
    "\n",
    "    print(classification_report(test_labels, y_pred))\n",
    "    \n",
    "\n",
    "    # classification report (cr)\n",
    "    cr = classification_report(test_labels, y_pred, output_dict=True)\n",
    "    recall_list.append(cr['vul']['recall'])\n",
    "    f1_list.append(cr['vul']['f1-score'])\n",
    "\n",
    "    ac = metrics.accuracy_score(test_labels, y_pred)\n",
    "    accuracies.append(ac)\n",
    "\n",
    "    # obtain fp and fn\n",
    "    fn, fp = get_fn_fp(y_pred, test_labels)\n",
    "    print('fn is: {0}, fp is: {1}'.format(fn, fp))\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "    \n",
    "    print(\"\\nAccuracy of prediction is: {}\".format(ac))\n",
    "    print(\"========================================================\")\n",
    "    print(\"========================================================\")\n",
    "\n",
    "\n",
    "print(\"*********************************************************\")\n",
    "print(\"\\nAverage accuracy: {}\".format(sum(accuracies) / len(accuracies)))\n",
    "\n",
    "perfs_dict['lr']['fp'] = sum(fp_list) / len(fn_list)\n",
    "perfs_dict['lr']['fn'] = sum(fn_list) / len(fn_list)\n",
    "perfs_dict['lr']['ac'] = sum(accuracies) / len(accuracies)\n",
    "perfs_dict['lr']['f1'] = sum(f1_list) / len(f1_list)\n",
    "perfs_dict['lr']['recall'] = sum(recall_list) / len(recall_list)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print(perfs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [  0   3   5   8   9  10  11  12  13  14  15  16  17  18  19  20  21  23\n",
      "  24  25  26  28  29  30  31  32  33  34  35  37  38  39  40  41  42  43\n",
      "  44  45  46  47  48  49  50  51  52  55  56  57  58  59  60  61  62  64\n",
      "  67  68  70  71  74  75  76  77  78  79  80  81  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  99 100 101 102 103 104] \n",
      "TEST: [ 1  2  4  6  7 22 27 36 53 54 63 65 66 69 72 73 82 83 84 97 98]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.91      1.00      0.95        10\n",
      "         vul       1.00      0.91      0.95        11\n",
      "\n",
      "    accuracy                           0.95        21\n",
      "   macro avg       0.95      0.95      0.95        21\n",
      "weighted avg       0.96      0.95      0.95        21\n",
      "\n",
      "fn is: 0.09090909090909091, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.9523809523809523\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   4   5   6   7   8   9  13  14  15  16  18  19  20  21  22\n",
      "  24  25  26  27  28  29  30  32  33  34  35  36  38  40  41  43  44  45\n",
      "  46  47  50  51  52  53  54  55  56  57  58  59  60  61  62  63  65  66\n",
      "  67  68  69  70  72  73  75  76  77  78  80  82  83  84  85  86  87  88\n",
      "  89  90  91  92  93  95  96  97  98  99 100 101] \n",
      "TEST: [  3  10  11  12  17  23  31  37  39  42  48  49  64  71  74  79  81  94\n",
      " 102 103 104]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.82      0.82      0.82        11\n",
      "         vul       0.80      0.80      0.80        10\n",
      "\n",
      "    accuracy                           0.81        21\n",
      "   macro avg       0.81      0.81      0.81        21\n",
      "weighted avg       0.81      0.81      0.81        21\n",
      "\n",
      "fn is: 0.2, fp is: 0.18181818181818182\n",
      "\n",
      "Accuracy of prediction is: 0.8095238095238095\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  16  17  18  19  21  22\n",
      "  23  25  26  27  28  29  31  33  34  36  37  39  40  41  42  43  45  46\n",
      "  47  48  49  52  53  54  55  56  57  59  60  61  62  63  64  65  66  67\n",
      "  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86\n",
      "  87  90  91  92  94  97  98  99 100 102 103 104] \n",
      "TEST: [  5  13  14  15  20  24  30  32  35  38  44  50  51  58  68  88  89  93\n",
      "  95  96 101]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.69      1.00      0.82         9\n",
      "         vul       1.00      0.67      0.80        12\n",
      "\n",
      "    accuracy                           0.81        21\n",
      "   macro avg       0.85      0.83      0.81        21\n",
      "weighted avg       0.87      0.81      0.81        21\n",
      "\n",
      "fn is: 0.3333333333333333, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.8095238095238095\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  17  18  19\n",
      "  20  21  22  23  24  27  29  30  31  32  34  35  36  37  38  39  40  41\n",
      "  42  44  45  48  49  50  51  52  53  54  57  58  60  63  64  65  66  67\n",
      "  68  69  70  71  72  73  74  76  77  79  81  82  83  84  86  88  89  90\n",
      "  92  93  94  95  96  97  98 100 101 102 103 104] \n",
      "TEST: [ 8 16 25 26 28 33 43 46 47 55 56 59 61 62 75 78 80 85 87 91 99]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.93      1.00      0.96        13\n",
      "         vul       1.00      0.88      0.93         8\n",
      "\n",
      "    accuracy                           0.95        21\n",
      "   macro avg       0.96      0.94      0.95        21\n",
      "weighted avg       0.96      0.95      0.95        21\n",
      "\n",
      "fn is: 0.125, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.9523809523809523\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  20  22\n",
      "  23  24  25  26  27  28  30  31  32  33  35  36  37  38  39  42  43  44\n",
      "  46  47  48  49  50  51  53  54  55  56  58  59  61  62  63  64  65  66\n",
      "  68  69  71  72  73  74  75  78  79  80  81  82  83  84  85  87  88  89\n",
      "  91  93  94  95  96  97  98  99 101 102 103 104] \n",
      "TEST: [  0   9  18  19  21  29  34  40  41  45  52  57  60  67  70  76  77  86\n",
      "  90  92 100]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.81      1.00      0.90        13\n",
      "         vul       1.00      0.62      0.77         8\n",
      "\n",
      "    accuracy                           0.86        21\n",
      "   macro avg       0.91      0.81      0.83        21\n",
      "weighted avg       0.88      0.86      0.85        21\n",
      "\n",
      "fn is: 0.375, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.8571428571428571\n",
      "========================================================\n",
      "========================================================\n",
      "*********************************************************\n",
      "\n",
      "Average accuracy: 0.8761904761904761\n",
      "---------------------------------------------------------\n",
      "{'nb': {}, 'lr': {'fp': 0.0, 'fn': 0.2578787878787879, 'ac': 0.8761904761904761, 'f1': 0.8457759784075574, 'recall': 0.7421212121212122}, 'knn': {'fp': 0.03636363636363636, 'fn': 0.22484848484848485, 'ac': 0.8761904761904761, 'f1': 0.850989010989011, 'recall': 0.7751515151515151}, 'svm': {}, 'rf': {'fp': 0.038181818181818185, 'fn': 0.06166666666666667, 'ac': 0.9523809523809523, 'f1': 0.9492753623188406, 'recall': 0.9383333333333332}}\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "# trying to scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "labels = list(df['label'])\n",
    "\n",
    "\n",
    "accuracies = list()\n",
    "fn_list = list()\n",
    "fp_list = list()\n",
    "recall_list = list()\n",
    "f1_list = list()\n",
    "\n",
    "counter = 0\n",
    "for train_indices, test_indices in kf.split(df):\n",
    "    train_indices = pd.read_csv(\"data/processing_steps/train_indices_{}.csv\".format(counter))\n",
    "    test_indices = pd.read_csv(\"data/processing_steps/test_indices_{}.csv\".format(counter))\n",
    "    \n",
    "    train_indices.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "    test_indices.drop(columns=[\"Unnamed: 0\"], inplace=True) \n",
    "\n",
    "    train_indices = train_indices['0'].to_numpy()\n",
    "    test_indices = test_indices['0'].to_numpy()\n",
    "\n",
    "    print(\"TRAIN:\", train_indices, \"\\nTEST:\", test_indices)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "    \n",
    "    train_df = df.loc[train_indices, :].copy()\n",
    "    test_df = df.loc[test_indices, :].copy()\n",
    "    \n",
    "    train_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    train_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    \n",
    "    test_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    test_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    train_labels = list(train_df['label'])\n",
    "    train_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    test_labels = list(test_df['label'])\n",
    "    test_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    # scaling:\n",
    "    scaler.fit(train_df)\n",
    "    train_df = scaler.transform(train_df)\n",
    "    test_df = scaler.transform(test_df)\n",
    "\n",
    "    classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "    # training\n",
    "    classifier.fit(train_df, train_labels)\n",
    "\n",
    "    # prediction\n",
    "    y_pred = classifier.predict(test_df)\n",
    "    #print(confusion_matrix(test_labels, y_pred))\n",
    "    print(classification_report(test_labels, y_pred))  \n",
    "\n",
    "    # classification report (cr)\n",
    "    cr = classification_report(test_labels, y_pred, output_dict=True)\n",
    "    recall_list.append(cr['vul']['recall'])\n",
    "    f1_list.append(cr['vul']['f1-score'])\n",
    "\n",
    "    ac = metrics.accuracy_score(test_labels, y_pred)\n",
    "    accuracies.append(ac)\n",
    "\n",
    "    # obtain fp and fn\n",
    "    fn, fp = get_fn_fp(y_pred, test_labels)\n",
    "    print('fn is: {0}, fp is: {1}'.format(fn, fp))\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)  \n",
    "\n",
    "    # measuring performance\n",
    "    ac = metrics.accuracy_score(test_labels, y_pred)\n",
    "    accuracies.append(ac)\n",
    "    \n",
    "    print(\"\\nAccuracy of prediction is: {}\".format(ac))\n",
    "    print(\"========================================================\")\n",
    "    print(\"========================================================\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"*********************************************************\")\n",
    "print(\"\\nAverage accuracy: {}\".format(sum(accuracies) / len(accuracies)))\n",
    "\n",
    "perfs_dict['knn']['fp'] = sum(fp_list) / len(fp_list)\n",
    "perfs_dict['knn']['fn'] = sum(fn_list) / len(fp_list)\n",
    "perfs_dict['knn']['ac'] = sum(accuracies) / len(accuracies)\n",
    "perfs_dict['knn']['f1'] = sum(f1_list) / len(f1_list)\n",
    "perfs_dict['knn']['recall'] = sum(recall_list) / len(recall_list)\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print(perfs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03636363636363636\n",
      "0.22484848484848485\n",
      "{'fp': 0.03636363636363636, 'fn': 0.22484848484848485, 'ac': 0.8761904761904761, 'f1': 0.850989010989011, 'recall': 0.7751515151515151}\n"
     ]
    }
   ],
   "source": [
    "print(perfs_dict['knn']['fp'])\n",
    "print(perfs_dict['knn']['fn'])\n",
    "print(perfs_dict['knn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [  0   3   5   8   9  10  11  12  13  14  15  16  17  18  19  20  21  23\n",
      "  24  25  26  28  29  30  31  32  33  34  35  37  38  39  40  41  42  43\n",
      "  44  45  46  47  48  49  50  51  52  55  56  57  58  59  60  61  62  64\n",
      "  67  68  70  71  74  75  76  77  78  79  80  81  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  99 100 101 102 103 104] \n",
      "TEST: [ 1  2  4  6  7 22 27 36 53 54 63 65 66 69 72 73 82 83 84 97 98]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.83      1.00      0.91        10\n",
      "         vul       1.00      0.82      0.90        11\n",
      "\n",
      "    accuracy                           0.90        21\n",
      "   macro avg       0.92      0.91      0.90        21\n",
      "weighted avg       0.92      0.90      0.90        21\n",
      "\n",
      "fn is: 0.18181818181818182, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.9047619047619048\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   4   5   6   7   8   9  13  14  15  16  18  19  20  21  22\n",
      "  24  25  26  27  28  29  30  32  33  34  35  36  38  40  41  43  44  45\n",
      "  46  47  50  51  52  53  54  55  56  57  58  59  60  61  62  63  65  66\n",
      "  67  68  69  70  72  73  75  76  77  78  80  82  83  84  85  86  87  88\n",
      "  89  90  91  92  93  95  96  97  98  99 100 101] \n",
      "TEST: [  3  10  11  12  17  23  31  37  39  42  48  49  64  71  74  79  81  94\n",
      " 102 103 104]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.92      1.00      0.96        11\n",
      "         vul       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.95        21\n",
      "   macro avg       0.96      0.95      0.95        21\n",
      "weighted avg       0.96      0.95      0.95        21\n",
      "\n",
      "fn is: 0.1, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.9523809523809523\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  16  17  18  19  21  22\n",
      "  23  25  26  27  28  29  31  33  34  36  37  39  40  41  42  43  45  46\n",
      "  47  48  49  52  53  54  55  56  57  59  60  61  62  63  64  65  66  67\n",
      "  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86\n",
      "  87  90  91  92  94  97  98  99 100 102 103 104] \n",
      "TEST: [  5  13  14  15  20  24  30  32  35  38  44  50  51  58  68  88  89  93\n",
      "  95  96 101]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.67      0.89      0.76         9\n",
      "         vul       0.89      0.67      0.76        12\n",
      "\n",
      "    accuracy                           0.76        21\n",
      "   macro avg       0.78      0.78      0.76        21\n",
      "weighted avg       0.79      0.76      0.76        21\n",
      "\n",
      "fn is: 0.3333333333333333, fp is: 0.1111111111111111\n",
      "\n",
      "Accuracy of prediction is: 0.7619047619047619\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  17  18  19\n",
      "  20  21  22  23  24  27  29  30  31  32  34  35  36  37  38  39  40  41\n",
      "  42  44  45  48  49  50  51  52  53  54  57  58  60  63  64  65  66  67\n",
      "  68  69  70  71  72  73  74  76  77  79  81  82  83  84  86  88  89  90\n",
      "  92  93  94  95  96  97  98 100 101 102 103 104] \n",
      "TEST: [ 8 16 25 26 28 33 43 46 47 55 56 59 61 62 75 78 80 85 87 91 99]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.92      0.92      0.92        13\n",
      "         vul       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.90        21\n",
      "   macro avg       0.90      0.90      0.90        21\n",
      "weighted avg       0.90      0.90      0.90        21\n",
      "\n",
      "fn is: 0.125, fp is: 0.07692307692307693\n",
      "\n",
      "Accuracy of prediction is: 0.9047619047619048\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  20  22\n",
      "  23  24  25  26  27  28  30  31  32  33  35  36  37  38  39  42  43  44\n",
      "  46  47  48  49  50  51  53  54  55  56  58  59  61  62  63  64  65  66\n",
      "  68  69  71  72  73  74  75  78  79  80  81  82  83  84  85  87  88  89\n",
      "  91  93  94  95  96  97  98  99 101 102 103 104] \n",
      "TEST: [  0   9  18  19  21  29  34  40  41  45  52  57  60  67  70  76  77  86\n",
      "  90  92 100]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.75      0.92      0.83        13\n",
      "         vul       0.80      0.50      0.62         8\n",
      "\n",
      "    accuracy                           0.76        21\n",
      "   macro avg       0.78      0.71      0.72        21\n",
      "weighted avg       0.77      0.76      0.75        21\n",
      "\n",
      "fn is: 0.5, fp is: 0.07692307692307693\n",
      "\n",
      "Accuracy of prediction is: 0.7619047619047619\n",
      "========================================================\n",
      "========================================================\n",
      "*********************************************************\n",
      "\n",
      "Average accuracy: 0.8571428571428573\n",
      "---------------------------------------------------------\n",
      "{'nb': {'fp': 0.05299145299145299, 'fn': 0.24803030303030305, 'ac': 0.8571428571428573, 'f1': 0.8199315596684018, 'recall': 0.751969696969697}, 'lr': {'fp': 0.0, 'fn': 0.2578787878787879, 'ac': 0.8761904761904761, 'f1': 0.8457759784075574, 'recall': 0.7421212121212122}, 'knn': {'fp': 0.03636363636363636, 'fn': 0.22484848484848485, 'ac': 0.8761904761904761, 'f1': 0.850989010989011, 'recall': 0.7751515151515151}, 'svm': {}, 'rf': {'fp': 0.038181818181818185, 'fn': 0.06166666666666667, 'ac': 0.9523809523809523, 'f1': 0.9492753623188406, 'recall': 0.9383333333333332}}\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "# trying to scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "labels = list(df['label'])\n",
    "\n",
    "\n",
    "accuracies = list()\n",
    "fn_list = list()\n",
    "fp_list = list()\n",
    "recall_list = list()\n",
    "f1_list = list()\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for train_indices, test_indices in kf.split(df):\n",
    "    train_indices = pd.read_csv(\"data/processing_steps/train_indices_{}.csv\".format(counter))\n",
    "    test_indices = pd.read_csv(\"data/processing_steps/test_indices_{}.csv\".format(counter))\n",
    "    \n",
    "    train_indices.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "    test_indices.drop(columns=[\"Unnamed: 0\"], inplace=True) \n",
    "\n",
    "    train_indices = train_indices['0'].to_numpy()\n",
    "    test_indices = test_indices['0'].to_numpy()\n",
    "\n",
    "    print(\"TRAIN:\", train_indices, \"\\nTEST:\", test_indices)\n",
    "\n",
    "    counter += 1\n",
    "    \n",
    "    train_df = df.loc[train_indices, :].copy()\n",
    "    test_df = df.loc[test_indices, :].copy()\n",
    "    \n",
    "    train_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    train_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    \n",
    "    test_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    test_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    train_labels = list(train_df['label'])\n",
    "    train_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    test_labels = list(test_df['label'])\n",
    "    test_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    # scaling:\n",
    "    scaler.fit(train_df)\n",
    "    #train_df = scaler.transform(train_df)\n",
    "    #test_df = scaler.transform(test_df)\n",
    "\n",
    "    # Gaussian Naive Bayes classier:\n",
    "    gnb = GaussianNB()\n",
    "\n",
    "    # training\n",
    "    gnb.fit(train_df, train_labels)\n",
    "\n",
    "    # prediction\n",
    "    y_pred = gnb.predict(test_df)\n",
    "    print(classification_report(test_labels, y_pred))    \n",
    "\n",
    "\n",
    "    # classification report (cr)\n",
    "    cr = classification_report(test_labels, y_pred, output_dict=True)\n",
    "    recall_list.append(cr['vul']['recall'])\n",
    "    f1_list.append(cr['vul']['f1-score'])\n",
    "\n",
    "    ac = metrics.accuracy_score(test_labels, y_pred)\n",
    "    accuracies.append(ac)\n",
    "\n",
    "\n",
    "    # obtain fp and fn\n",
    "    fn, fp = get_fn_fp(y_pred, test_labels)\n",
    "    print('fn is: {0}, fp is: {1}'.format(fn, fp))\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "\n",
    "\n",
    "    # measuring performance\n",
    "    ac = metrics.accuracy_score(test_labels, y_pred)\n",
    "    accuracies.append(ac)\n",
    "    \n",
    "    print(\"\\nAccuracy of prediction is: {}\".format(ac))\n",
    "    print(\"========================================================\")\n",
    "    print(\"========================================================\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"*********************************************************\")\n",
    "print(\"\\nAverage accuracy: {}\".format(sum(accuracies) / len(accuracies)))\n",
    "\n",
    "perfs_dict['nb']['fp'] = sum(fp_list) / len(fp_list)\n",
    "perfs_dict['nb']['fn'] = sum(fn_list) / len(fn_list)\n",
    "perfs_dict['nb']['ac'] = sum(accuracies) / len(accuracies)\n",
    "perfs_dict['nb']['f1'] = sum(f1_list) / len(f1_list)\n",
    "perfs_dict['nb']['recall'] = sum(recall_list) / len(recall_list)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print(perfs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [  0   3   5   8   9  10  11  12  13  14  15  16  17  18  19  20  21  23\n",
      "  24  25  26  28  29  30  31  32  33  34  35  37  38  39  40  41  42  43\n",
      "  44  45  46  47  48  49  50  51  52  55  56  57  58  59  60  61  62  64\n",
      "  67  68  70  71  74  75  76  77  78  79  80  81  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  99 100 101 102 103 104] \n",
      "TEST: [ 1  2  4  6  7 22 27 36 53 54 63 65 66 69 72 73 82 83 84 97 98]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.83      1.00      0.91        10\n",
      "         vul       1.00      0.82      0.90        11\n",
      "\n",
      "    accuracy                           0.90        21\n",
      "   macro avg       0.92      0.91      0.90        21\n",
      "weighted avg       0.92      0.90      0.90        21\n",
      "\n",
      "fn is: 0.18181818181818182, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.9047619047619048\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   4   5   6   7   8   9  13  14  15  16  18  19  20  21  22\n",
      "  24  25  26  27  28  29  30  32  33  34  35  36  38  40  41  43  44  45\n",
      "  46  47  50  51  52  53  54  55  56  57  58  59  60  61  62  63  65  66\n",
      "  67  68  69  70  72  73  75  76  77  78  80  82  83  84  85  86  87  88\n",
      "  89  90  91  92  93  95  96  97  98  99 100 101] \n",
      "TEST: [  3  10  11  12  17  23  31  37  39  42  48  49  64  71  74  79  81  94\n",
      " 102 103 104]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.92      1.00      0.96        11\n",
      "         vul       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.95        21\n",
      "   macro avg       0.96      0.95      0.95        21\n",
      "weighted avg       0.96      0.95      0.95        21\n",
      "\n",
      "fn is: 0.1, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.9523809523809523\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  16  17  18  19  21  22\n",
      "  23  25  26  27  28  29  31  33  34  36  37  39  40  41  42  43  45  46\n",
      "  47  48  49  52  53  54  55  56  57  59  60  61  62  63  64  65  66  67\n",
      "  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86\n",
      "  87  90  91  92  94  97  98  99 100 102 103 104] \n",
      "TEST: [  5  13  14  15  20  24  30  32  35  38  44  50  51  58  68  88  89  93\n",
      "  95  96 101]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.64      1.00      0.78         9\n",
      "         vul       1.00      0.58      0.74        12\n",
      "\n",
      "    accuracy                           0.76        21\n",
      "   macro avg       0.82      0.79      0.76        21\n",
      "weighted avg       0.85      0.76      0.76        21\n",
      "\n",
      "fn is: 0.4166666666666667, fp is: 0.0\n",
      "\n",
      "Accuracy of prediction is: 0.7619047619047619\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  17  18  19\n",
      "  20  21  22  23  24  27  29  30  31  32  34  35  36  37  38  39  40  41\n",
      "  42  44  45  48  49  50  51  52  53  54  57  58  60  63  64  65  66  67\n",
      "  68  69  70  71  72  73  74  76  77  79  81  82  83  84  86  88  89  90\n",
      "  92  93  94  95  96  97  98 100 101 102 103 104] \n",
      "TEST: [ 8 16 25 26 28 33 43 46 47 55 56 59 61 62 75 78 80 85 87 91 99]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.92      0.85      0.88        13\n",
      "         vul       0.78      0.88      0.82         8\n",
      "\n",
      "    accuracy                           0.86        21\n",
      "   macro avg       0.85      0.86      0.85        21\n",
      "weighted avg       0.86      0.86      0.86        21\n",
      "\n",
      "fn is: 0.125, fp is: 0.15384615384615385\n",
      "\n",
      "Accuracy of prediction is: 0.8571428571428571\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  20  22\n",
      "  23  24  25  26  27  28  30  31  32  33  35  36  37  38  39  42  43  44\n",
      "  46  47  48  49  50  51  53  54  55  56  58  59  61  62  63  64  65  66\n",
      "  68  69  71  72  73  74  75  78  79  80  81  82  83  84  85  87  88  89\n",
      "  91  93  94  95  96  97  98  99 101 102 103 104] \n",
      "TEST: [  0   9  18  19  21  29  34  40  41  45  52  57  60  67  70  76  77  86\n",
      "  90  92 100]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.80      0.92      0.86        13\n",
      "         vul       0.83      0.62      0.71         8\n",
      "\n",
      "    accuracy                           0.81        21\n",
      "   macro avg       0.82      0.77      0.79        21\n",
      "weighted avg       0.81      0.81      0.80        21\n",
      "\n",
      "fn is: 0.375, fp is: 0.07692307692307693\n",
      "\n",
      "Accuracy of prediction is: 0.8095238095238095\n",
      "========================================================\n",
      "========================================================\n",
      "*********************************************************\n",
      "\n",
      "Average accuracy: 0.8571428571428571\n",
      "---------------------------------------------------------\n",
      "{'nb': {'fp': 0.05299145299145299, 'fn': 0.24803030303030305, 'ac': 0.8571428571428573, 'f1': 0.8199315596684018, 'recall': 0.751969696969697}, 'lr': {'fp': 0.0, 'fn': 0.2578787878787879, 'ac': 0.8761904761904761, 'f1': 0.8457759784075574, 'recall': 0.7421212121212122}, 'knn': {'fp': 0.03636363636363636, 'fn': 0.22484848484848485, 'ac': 0.8761904761904761, 'f1': 0.850989010989011, 'recall': 0.7751515151515151}, 'svm': {'fp': 0.046153846153846156, 'fn': 0.2396969696969697, 'ac': 0.8571428571428571, 'f1': 0.8244051304732419, 'recall': 0.7603030303030304}, 'rf': {'fp': 0.038181818181818185, 'fn': 0.06166666666666667, 'ac': 0.9523809523809523, 'f1': 0.9492753623188406, 'recall': 0.9383333333333332}}\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier\n",
    "\n",
    "# trying to scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "labels = list(df['label'])\n",
    "\n",
    "\n",
    "accuracies = list()\n",
    "fn_list = list()\n",
    "fp_list = list()\n",
    "recall_list = list()\n",
    "f1_list = list()\n",
    "\n",
    "\n",
    "counter = 0 \n",
    "for train_indices, test_indices in kf.split(df):\n",
    "    train_indices = pd.read_csv(\"data/processing_steps/train_indices_{}.csv\".format(counter))\n",
    "    test_indices = pd.read_csv(\"data/processing_steps/test_indices_{}.csv\".format(counter))\n",
    "    \n",
    "    train_indices.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "    test_indices.drop(columns=[\"Unnamed: 0\"], inplace=True) \n",
    "\n",
    "    train_indices = train_indices['0'].to_numpy()\n",
    "    test_indices = test_indices['0'].to_numpy()\n",
    "\n",
    "    print(\"TRAIN:\", train_indices, \"\\nTEST:\", test_indices)\n",
    "\n",
    "    counter += 1\n",
    "    \n",
    "    train_df = df.loc[train_indices, :].copy()\n",
    "    test_df = df.loc[test_indices, :].copy()\n",
    "    \n",
    "    train_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    train_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    \n",
    "    test_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    test_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    train_labels = list(train_df['label'])\n",
    "    train_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    test_labels = list(test_df['label'])\n",
    "    test_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    # scaling:\n",
    "    scaler.fit(train_df)\n",
    "    train_df = scaler.transform(train_df)\n",
    "    test_df = scaler.transform(test_df)\n",
    "\n",
    "    # SVM classier:\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "\n",
    "    # training\n",
    "    clf.fit(train_df, train_labels)\n",
    "\n",
    "    # prediction\n",
    "    y_pred = clf.predict(test_df)\n",
    "    print(classification_report(test_labels, y_pred))    \n",
    "\n",
    "    # classification report (cr)\n",
    "    cr = classification_report(test_labels, y_pred, output_dict=True)\n",
    "    recall_list.append(cr['vul']['recall'])\n",
    "    f1_list.append(cr['vul']['f1-score'])\n",
    "\n",
    "    ac = metrics.accuracy_score(test_labels, y_pred)\n",
    "    accuracies.append(ac)\n",
    "\n",
    "    # obtain fp and fn\n",
    "    fn, fp = get_fn_fp(y_pred, test_labels)\n",
    "    print('fn is: {0}, fp is: {1}'.format(fn, fp))\n",
    "    fp_list.append(fp)\n",
    "    fn_list.append(fn)\n",
    "\n",
    "    # measuring performance\n",
    "    ac = metrics.accuracy_score(test_labels, y_pred)\n",
    "    accuracies.append(ac)\n",
    "    \n",
    "    print(\"\\nAccuracy of prediction is: {}\".format(ac))\n",
    "    print(\"========================================================\")\n",
    "    print(\"========================================================\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"*********************************************************\")\n",
    "print(\"\\nAverage accuracy: {}\".format(sum(accuracies) / len(accuracies)))\n",
    "\n",
    "perfs_dict['svm']['fp'] = sum(fp_list) / len(fp_list)\n",
    "perfs_dict['svm']['fn'] = sum(fn_list) / len(fn_list)\n",
    "perfs_dict['svm']['ac'] = sum(accuracies) / len(accuracies)\n",
    "perfs_dict['svm']['f1'] = sum(f1_list) / len(f1_list)\n",
    "perfs_dict['svm']['recall'] = sum(recall_list) / len(recall_list)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "print(perfs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plotting the fpr and fnr\n",
    "\n",
    "\n",
    "fp_ = list()\n",
    "fn_ = list()\n",
    "f1_ = list()\n",
    "ac_ = list()\n",
    "recall_ = list()\n",
    "\n",
    "for classifier, classifier_dict in perfs_dict.items():\n",
    "    \n",
    "    fp_.append(round(classifier_dict['fp'], 2))\n",
    "    fn_.append(round(classifier_dict['fn'], 2))\n",
    "    f1_.append(round(classifier_dict['f1'], 2))\n",
    "    ac_.append(round(classifier_dict['ac'], 2))\n",
    "    recall_.append(round(classifier_dict['recall'], 2))\n",
    "\n",
    "\n",
    "labels = ['NB', 'Logistic Regression', 'K-NN', 'SVM', 'RF']\n",
    "\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, fp_, width, label='Mean FP', color='mediumseagreen')\n",
    "rects2 = ax.bar(x + width/2, fn_, width, label='Mean FN')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Percentage')\n",
    "#ax.set_title('Number of False Positive and False Negative')\n",
    "ax.set_xticks(x)\n",
    "ax.set_ylim(top=1.0)\n",
    "ax.set_xticklabels(labels)\n",
    "#ax.legend()\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.13),\n",
    "          fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 0),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 5)\n",
    "fig.tight_layout()\n",
    "plt.savefig('fp_fn_no_callstack.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ploting the f1_score, accuracy, and recall\n",
    "\n",
    "\n",
    "\n",
    "labels = ['NB', 'Logistic Regression', 'K-NN', 'SVM', 'RF']\n",
    "\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, ac_, width, label='Accuracy', color='mediumseagreen')\n",
    "rects2 = ax.bar(x + width/2, f1_, width, label='F1 Score')\n",
    "rects3 = ax.bar(x + 3*width/2, recall_, width, label='Recall')\n",
    "ax.set_ylim(bottom=0.60, top=1.0)\n",
    "#ax.set_ylim(bottom=0.60)\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Mean Performance')\n",
    "#ax.set_title('Number of False Positive and False Negative')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "#ax.legend()\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.13),\n",
    "          fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def autolabel(rects, rect_num=None):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        \n",
    "        xytext=(0, 0)\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=xytext,  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', rotation=35)\n",
    "        \n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2, rect_num=2)\n",
    "autolabel(rects3)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 5)\n",
    "fig.tight_layout()\n",
    "plt.savefig('ac_f1_recall_no_callstack.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb': {'fp': 0.05299145299145299, 'fn': 0.24803030303030305, 'ac': 0.8571428571428573, 'f1': 0.8199315596684018, 'recall': 0.751969696969697}, 'lr': {'fp': 0.0, 'fn': 0.2578787878787879, 'ac': 0.8761904761904761, 'f1': 0.8457759784075574, 'recall': 0.7421212121212122}, 'knn': {'fp': 0.03636363636363636, 'fn': 0.22484848484848485, 'ac': 0.8761904761904761, 'f1': 0.850989010989011, 'recall': 0.7751515151515151}, 'svm': {'fp': 0.046153846153846156, 'fn': 0.2396969696969697, 'ac': 0.8571428571428571, 'f1': 0.8244051304732419, 'recall': 0.7603030303030304}, 'rf': {'fp': 0.038181818181818185, 'fn': 0.06166666666666667, 'ac': 0.9523809523809523, 'f1': 0.9492753623188406, 'recall': 0.9383333333333332}}\n"
     ]
    }
   ],
   "source": [
    "print(perfs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
