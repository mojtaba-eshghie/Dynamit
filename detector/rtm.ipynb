{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "import string\n",
    "from sklearn.model_selection import KFold\n",
    "from decimal import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import svm"
   ]
  },
  {
   "source": [
    "## Cleaning the Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions.csv and transactions2.csv are data gathered from the same experiment. However, transactions2.csv is the result of the one that has more randomness to the generated transactions.\n",
    "\n",
    "df = pd.read_csv('data/transactions2.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions may be used in case we need to do a data transformation into categorical data.\n",
    "\n",
    "def gas_bining(org_df):\n",
    "    df = org_df.copy()\n",
    "    \n",
    "    df['gas_used'] = pd.cut(df['gas_used'], bins=[i * 100000 for i in range(0, 20)], labels=list(string.ascii_uppercase[:19]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_balance_delta(org_df):\n",
    "    df = org_df.copy()\n",
    "\n",
    "\n",
    "    for i in range(0, df.shape[0]):\n",
    "\n",
    "        if int(df.at[i, 'victim_balance_delta']) == 0:\n",
    "            df.at[i, 'victim_balance_delta'] = 'zero'\n",
    "\n",
    "        elif int(df.at[i, 'victim_balance_delta']) > 0:\n",
    "            df.at[i, 'victim_balance_delta'] = 'positive'\n",
    "\n",
    "        elif int(df.at[i, 'victim_balance_delta']) < 0:\n",
    "            df.at[i, 'victim_balance_delta'] = 'negative'\n",
    "\n",
    "\n",
    "        if int(df.at[i, 'attacker_balance_delta']) == 0:\n",
    "            df.at[i, 'attacker_balance_delta'] = 'zero'\n",
    "\n",
    "        elif int(df.at[i, 'attacker_balance_delta']) > 0:\n",
    "            df.at[i, 'attacker_balance_delta'] = 'positive'\n",
    "\n",
    "        elif int(df.at[i, 'attacker_balance_delta']) < 0:\n",
    "            df.at[i, 'attacker_balance_delta'] = 'negative'\n",
    "\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def call_stack_depth_bining(org_df):\n",
    "    df = org_df.copy()\n",
    "\n",
    "    df['call_stack_depth'] = pd.cut(df['call_stack_depth'], bins=[i for i in range(0, 20)], labels=list(string.ascii_uppercase[:19]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "new_df = gas_bining(df)\n",
    "new_df = transform_balance_delta(new_df)\n",
    "new_df = call_stack_depth_bining(new_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def get_ys_from_prediction(prediction_df):\n",
    "    df = prediction_df.copy()\n",
    "    ys = list()\n",
    "\n",
    "    col_list = df.columns\n",
    "    for index, row in df.iterrows():\n",
    "        max_prob = row.max()\n",
    "        for col in col_list:\n",
    "            if row[col] == max_prob:\n",
    "                predicted = col\n",
    "                break\n",
    "        ys.append(predicted)\n",
    "    \n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An slightly manipulated version of the Random Forest written for course assignment\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.column_filter = None\n",
    "        self.imputation = None \n",
    "        self.one_hot = None \n",
    "        self.labels = None \n",
    "        self.model = None\n",
    "        \n",
    "\n",
    "        # HELPER\n",
    "        self.no_trees = None\n",
    "\n",
    "        # DEBUG\n",
    "        self.tree_predictions_list = None\n",
    "\n",
    "    \n",
    "    def fit(self, df, no_trees=100):\n",
    "        self.no_trees = no_trees\n",
    "        df = df.copy()\n",
    "\n",
    "        # initializing our random forest as a list of all our generated trees:\n",
    "        self.model = list()\n",
    "        \n",
    "        y = df['label'].values\n",
    "        df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "        \n",
    "        #df, self.one_hot = create_one_hot(df)\n",
    "        #df, self.column_filter = create_column_filter(df)\n",
    "        #df, self.imputation = create_imputation(df)\n",
    "        \n",
    "        x = df.values\n",
    "\n",
    "        # total number of features:\n",
    "        F_size = len(df.columns)\n",
    "\n",
    "\n",
    "        # just to use later!\n",
    "        df_with_classes = df.copy()\n",
    "        df_with_classes['label'] = pd.Series(y, index=df_with_classes.index)\n",
    "\n",
    "        def select_with_replacement():\n",
    "            '''\n",
    "            This function will return a newly selected with replacement sample.\n",
    "            '''\n",
    "            sample = pd.DataFrame(columns = df_with_classes.columns)\n",
    "            selections = np.random.choice(df_with_classes.shape[0], df_with_classes.shape[0], replace=True)\n",
    "            \n",
    "            for selection in selections:\n",
    "                sample = sample.append(df_with_classes.iloc[selection])\n",
    "                \n",
    "\n",
    "            return sample \n",
    "\n",
    "\n",
    "\n",
    "        for trees_i in range(0, no_trees):\n",
    "            \n",
    "            sample_df = select_with_replacement()\n",
    "            \n",
    "            y = sample_df['label'].values\n",
    "            sample_df.drop(columns=['label'], inplace=True)\n",
    "            x = sample_df.values\n",
    "\n",
    "            self.model.append(DecisionTreeClassifier(max_features='log2'))\n",
    "            self.model[-1].fit(x, y)\n",
    "\n",
    "            print('tree # {} is built,'.format(trees_i))\n",
    "\n",
    "\n",
    "            # I used these lines to get insight into how are my decision trees doing (commented)\n",
    "            '''\n",
    "            tree_desc = tree.export_graphviz(self.model[-1], out_file='1_forest/{}.dot'.format(trees_i), feature_names=list(sample_df.columns))\n",
    "            dot_data = tree.export_graphviz(self.model[-1], feature_names=list(sample_df.columns), class_names=[str(c) for c in self.model[-1].classes_], filled=True, rounded=True)\n",
    "            graph = graphviz.Source(dot_data)\n",
    "            graph.render('1_forest/{}.gv'.format(trees_i), view=False)  \n",
    "            '''\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def predict(self, df):\n",
    "        test_df = df.copy()\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        #test_df = apply_one_hot(test_df, self.one_hot)\n",
    "        #test_df = apply_column_filter(test_df, self.column_filter)\n",
    "        #test_df = apply_imputation(test_df, self.imputation)\n",
    "\n",
    "        test_x = test_df.values\n",
    "\n",
    "        # let's add all predictions from all trees to a single list so that we can use it later!\n",
    "        # each index in this list denotes the prediction of a tree\n",
    "        tree_predictions_list = list()\n",
    "\n",
    "        for dt in self.model:\n",
    "            tree_predictions = dt.predict_proba(test_x)\n",
    "            tree_predictions_list.append(tree_predictions)\n",
    "\n",
    "        self.tree_predictions_list = tree_predictions_list\n",
    "\n",
    "\n",
    "\n",
    "        predictions = pd.DataFrame(columns=['safe' ,'vul'])\n",
    "\n",
    "        # averaging for each instance:\n",
    "        for instance_index in range(0, len(test_x)):\n",
    "            \n",
    "            total_negative = 0\n",
    "            total_positive = 0\n",
    "            \n",
    "            for model_index in range(0, self.no_trees):\n",
    "                total_negative += tree_predictions_list[model_index][instance_index][0]\n",
    "                total_positive += tree_predictions_list[model_index][instance_index][1]\n",
    "            \n",
    "            \n",
    "\n",
    "            predictions = predictions.append({\n",
    "                'safe': Decimal(total_negative) / Decimal(self.no_trees),\n",
    "                'vul': Decimal(total_positive) / Decimal(self.no_trees)\n",
    "            }, ignore_index=True)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(original_df, correctlabels):\n",
    "    \n",
    "    df = original_df.copy()\n",
    "    \n",
    "    '''\n",
    "    Assumption: I assume that the accuracy is ratio `number of data instances \n",
    "    correctly classified / total no of data instances`\n",
    "    Furthermore, for an instance that does not have a label with highest probability,\n",
    "    I will choose the first label in column orders as the predicted label for that instance.\n",
    "    '''\n",
    "    \n",
    "    cor_pred = list() # correct predictions list\n",
    "    inc_pred = list() # incorrect predictions list\n",
    "    col_list = df.columns\n",
    "    for index, row in df.iterrows():\n",
    "        max_prob = row.max()\n",
    "        for col in col_list:\n",
    "            if row[col] == max_prob:\n",
    "                predicted = col\n",
    "                break\n",
    "        correct = correctlabels[index]\n",
    "        if correct == predicted:\n",
    "            cor_pred.append(index)\n",
    "        else:\n",
    "            inc_pred.append(index)\n",
    "    \n",
    "    return len(cor_pred)/ len(correctlabels)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN: [  1   2   3   4   5   6   7   9  10  11  12  13  15  17  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  41  42\n",
      "  44  45  46  48  49  51  52  53  54  56  58  59  60  61  63  64  65  66\n",
      "  67  69  71  72  73  74  75  76  77  78  79  80  81  83  84  85  86  88\n",
      "  90  91  92  93  95  96  97  98  99 100 102 103] \n",
      "TEST: [  0   8  14  16  18  39  40  43  47  50  55  57  62  68  70  82  87  89\n",
      "  94 101 104]\n",
      "tree # 0 is built,\n",
      "tree # 1 is built,\n",
      "tree # 2 is built,\n",
      "tree # 3 is built,\n",
      "tree # 4 is built,\n",
      "tree # 5 is built,\n",
      "tree # 6 is built,\n",
      "tree # 7 is built,\n",
      "tree # 8 is built,\n",
      "tree # 9 is built,\n",
      "tree # 10 is built,\n",
      "tree # 11 is built,\n",
      "tree # 12 is built,\n",
      "tree # 13 is built,\n",
      "tree # 14 is built,\n",
      "tree # 15 is built,\n",
      "tree # 16 is built,\n",
      "tree # 17 is built,\n",
      "tree # 18 is built,\n",
      "tree # 19 is built,\n",
      "tree # 20 is built,\n",
      "tree # 21 is built,\n",
      "tree # 22 is built,\n",
      "tree # 23 is built,\n",
      "tree # 24 is built,\n",
      "tree # 25 is built,\n",
      "tree # 26 is built,\n",
      "tree # 27 is built,\n",
      "tree # 28 is built,\n",
      "tree # 29 is built,\n",
      "tree # 30 is built,\n",
      "tree # 31 is built,\n",
      "tree # 32 is built,\n",
      "tree # 33 is built,\n",
      "tree # 34 is built,\n",
      "tree # 35 is built,\n",
      "tree # 36 is built,\n",
      "tree # 37 is built,\n",
      "tree # 38 is built,\n",
      "tree # 39 is built,\n",
      "tree # 40 is built,\n",
      "tree # 41 is built,\n",
      "tree # 42 is built,\n",
      "tree # 43 is built,\n",
      "tree # 44 is built,\n",
      "tree # 45 is built,\n",
      "tree # 46 is built,\n",
      "tree # 47 is built,\n",
      "tree # 48 is built,\n",
      "tree # 49 is built,\n",
      "tree # 50 is built,\n",
      "tree # 51 is built,\n",
      "tree # 52 is built,\n",
      "tree # 53 is built,\n",
      "tree # 54 is built,\n",
      "tree # 55 is built,\n",
      "tree # 56 is built,\n",
      "tree # 57 is built,\n",
      "tree # 58 is built,\n",
      "tree # 59 is built,\n",
      "tree # 60 is built,\n",
      "tree # 61 is built,\n",
      "tree # 62 is built,\n",
      "tree # 63 is built,\n",
      "tree # 64 is built,\n",
      "tree # 65 is built,\n",
      "tree # 66 is built,\n",
      "tree # 67 is built,\n",
      "tree # 68 is built,\n",
      "tree # 69 is built,\n",
      "tree # 70 is built,\n",
      "tree # 71 is built,\n",
      "tree # 72 is built,\n",
      "tree # 73 is built,\n",
      "tree # 74 is built,\n",
      "tree # 75 is built,\n",
      "tree # 76 is built,\n",
      "tree # 77 is built,\n",
      "tree # 78 is built,\n",
      "tree # 79 is built,\n",
      "tree # 80 is built,\n",
      "tree # 81 is built,\n",
      "tree # 82 is built,\n",
      "tree # 83 is built,\n",
      "tree # 84 is built,\n",
      "tree # 85 is built,\n",
      "tree # 86 is built,\n",
      "tree # 87 is built,\n",
      "tree # 88 is built,\n",
      "tree # 89 is built,\n",
      "tree # 90 is built,\n",
      "tree # 91 is built,\n",
      "tree # 92 is built,\n",
      "tree # 93 is built,\n",
      "tree # 94 is built,\n",
      "tree # 95 is built,\n",
      "tree # 96 is built,\n",
      "tree # 97 is built,\n",
      "tree # 98 is built,\n",
      "tree # 99 is built,\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       1.00      0.88      0.93         8\n",
      "         vul       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.95        21\n",
      "   macro avg       0.96      0.94      0.95        21\n",
      "weighted avg       0.96      0.95      0.95        21\n",
      "\n",
      "\n",
      "Accuracy of prediction is: 0.9523809523809523\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   4   5   6   7   8   9  10  11  14  16  18  20  21  22  23  24  25\n",
      "  26  27  28  29  31  32  34  35  36  37  39  40  43  44  45  47  48  49\n",
      "  50  51  52  53  54  55  57  59  60  62  63  64  65  66  67  68  70  71\n",
      "  72  73  74  75  76  77  78  79  80  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  97  98  99 100 101 102 103 104] \n",
      "TEST: [ 1  2  3 12 13 15 17 19 30 33 38 41 42 46 56 58 61 69 81 95 96]\n",
      "tree # 0 is built,\n",
      "tree # 1 is built,\n",
      "tree # 2 is built,\n",
      "tree # 3 is built,\n",
      "tree # 4 is built,\n",
      "tree # 5 is built,\n",
      "tree # 6 is built,\n",
      "tree # 7 is built,\n",
      "tree # 8 is built,\n",
      "tree # 9 is built,\n",
      "tree # 10 is built,\n",
      "tree # 11 is built,\n",
      "tree # 12 is built,\n",
      "tree # 13 is built,\n",
      "tree # 14 is built,\n",
      "tree # 15 is built,\n",
      "tree # 16 is built,\n",
      "tree # 17 is built,\n",
      "tree # 18 is built,\n",
      "tree # 19 is built,\n",
      "tree # 20 is built,\n",
      "tree # 21 is built,\n",
      "tree # 22 is built,\n",
      "tree # 23 is built,\n",
      "tree # 24 is built,\n",
      "tree # 25 is built,\n",
      "tree # 26 is built,\n",
      "tree # 27 is built,\n",
      "tree # 28 is built,\n",
      "tree # 29 is built,\n",
      "tree # 30 is built,\n",
      "tree # 31 is built,\n",
      "tree # 32 is built,\n",
      "tree # 33 is built,\n",
      "tree # 34 is built,\n",
      "tree # 35 is built,\n",
      "tree # 36 is built,\n",
      "tree # 37 is built,\n",
      "tree # 38 is built,\n",
      "tree # 39 is built,\n",
      "tree # 40 is built,\n",
      "tree # 41 is built,\n",
      "tree # 42 is built,\n",
      "tree # 43 is built,\n",
      "tree # 44 is built,\n",
      "tree # 45 is built,\n",
      "tree # 46 is built,\n",
      "tree # 47 is built,\n",
      "tree # 48 is built,\n",
      "tree # 49 is built,\n",
      "tree # 50 is built,\n",
      "tree # 51 is built,\n",
      "tree # 52 is built,\n",
      "tree # 53 is built,\n",
      "tree # 54 is built,\n",
      "tree # 55 is built,\n",
      "tree # 56 is built,\n",
      "tree # 57 is built,\n",
      "tree # 58 is built,\n",
      "tree # 59 is built,\n",
      "tree # 60 is built,\n",
      "tree # 61 is built,\n",
      "tree # 62 is built,\n",
      "tree # 63 is built,\n",
      "tree # 64 is built,\n",
      "tree # 65 is built,\n",
      "tree # 66 is built,\n",
      "tree # 67 is built,\n",
      "tree # 68 is built,\n",
      "tree # 69 is built,\n",
      "tree # 70 is built,\n",
      "tree # 71 is built,\n",
      "tree # 72 is built,\n",
      "tree # 73 is built,\n",
      "tree # 74 is built,\n",
      "tree # 75 is built,\n",
      "tree # 76 is built,\n",
      "tree # 77 is built,\n",
      "tree # 78 is built,\n",
      "tree # 79 is built,\n",
      "tree # 80 is built,\n",
      "tree # 81 is built,\n",
      "tree # 82 is built,\n",
      "tree # 83 is built,\n",
      "tree # 84 is built,\n",
      "tree # 85 is built,\n",
      "tree # 86 is built,\n",
      "tree # 87 is built,\n",
      "tree # 88 is built,\n",
      "tree # 89 is built,\n",
      "tree # 90 is built,\n",
      "tree # 91 is built,\n",
      "tree # 92 is built,\n",
      "tree # 93 is built,\n",
      "tree # 94 is built,\n",
      "tree # 95 is built,\n",
      "tree # 96 is built,\n",
      "tree # 97 is built,\n",
      "tree # 98 is built,\n",
      "tree # 99 is built,\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.85      1.00      0.92        11\n",
      "         vul       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.90        21\n",
      "   macro avg       0.92      0.90      0.90        21\n",
      "weighted avg       0.92      0.90      0.90        21\n",
      "\n",
      "\n",
      "Accuracy of prediction is: 0.9047619047619048\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   3   7   8   9  11  12  13  14  15  16  17  18  19  20  21\n",
      "  22  23  24  25  27  28  30  31  32  33  34  35  37  38  39  40  41  42\n",
      "  43  45  46  47  48  50  51  53  55  56  57  58  59  61  62  64  65  66\n",
      "  67  68  69  70  72  73  77  79  81  82  83  84  85  86  87  88  89  91\n",
      "  92  93  94  95  96  97  98  99 101 102 103 104] \n",
      "TEST: [  4   5   6  10  26  29  36  44  49  52  54  60  63  71  74  75  76  78\n",
      "  80  90 100]\n",
      "tree # 0 is built,\n",
      "tree # 1 is built,\n",
      "tree # 2 is built,\n",
      "tree # 3 is built,\n",
      "tree # 4 is built,\n",
      "tree # 5 is built,\n",
      "tree # 6 is built,\n",
      "tree # 7 is built,\n",
      "tree # 8 is built,\n",
      "tree # 9 is built,\n",
      "tree # 10 is built,\n",
      "tree # 11 is built,\n",
      "tree # 12 is built,\n",
      "tree # 13 is built,\n",
      "tree # 14 is built,\n",
      "tree # 15 is built,\n",
      "tree # 16 is built,\n",
      "tree # 17 is built,\n",
      "tree # 18 is built,\n",
      "tree # 19 is built,\n",
      "tree # 20 is built,\n",
      "tree # 21 is built,\n",
      "tree # 22 is built,\n",
      "tree # 23 is built,\n",
      "tree # 24 is built,\n",
      "tree # 25 is built,\n",
      "tree # 26 is built,\n",
      "tree # 27 is built,\n",
      "tree # 28 is built,\n",
      "tree # 29 is built,\n",
      "tree # 30 is built,\n",
      "tree # 31 is built,\n",
      "tree # 32 is built,\n",
      "tree # 33 is built,\n",
      "tree # 34 is built,\n",
      "tree # 35 is built,\n",
      "tree # 36 is built,\n",
      "tree # 37 is built,\n",
      "tree # 38 is built,\n",
      "tree # 39 is built,\n",
      "tree # 40 is built,\n",
      "tree # 41 is built,\n",
      "tree # 42 is built,\n",
      "tree # 43 is built,\n",
      "tree # 44 is built,\n",
      "tree # 45 is built,\n",
      "tree # 46 is built,\n",
      "tree # 47 is built,\n",
      "tree # 48 is built,\n",
      "tree # 49 is built,\n",
      "tree # 50 is built,\n",
      "tree # 51 is built,\n",
      "tree # 52 is built,\n",
      "tree # 53 is built,\n",
      "tree # 54 is built,\n",
      "tree # 55 is built,\n",
      "tree # 56 is built,\n",
      "tree # 57 is built,\n",
      "tree # 58 is built,\n",
      "tree # 59 is built,\n",
      "tree # 60 is built,\n",
      "tree # 61 is built,\n",
      "tree # 62 is built,\n",
      "tree # 63 is built,\n",
      "tree # 64 is built,\n",
      "tree # 65 is built,\n",
      "tree # 66 is built,\n",
      "tree # 67 is built,\n",
      "tree # 68 is built,\n",
      "tree # 69 is built,\n",
      "tree # 70 is built,\n",
      "tree # 71 is built,\n",
      "tree # 72 is built,\n",
      "tree # 73 is built,\n",
      "tree # 74 is built,\n",
      "tree # 75 is built,\n",
      "tree # 76 is built,\n",
      "tree # 77 is built,\n",
      "tree # 78 is built,\n",
      "tree # 79 is built,\n",
      "tree # 80 is built,\n",
      "tree # 81 is built,\n",
      "tree # 82 is built,\n",
      "tree # 83 is built,\n",
      "tree # 84 is built,\n",
      "tree # 85 is built,\n",
      "tree # 86 is built,\n",
      "tree # 87 is built,\n",
      "tree # 88 is built,\n",
      "tree # 89 is built,\n",
      "tree # 90 is built,\n",
      "tree # 91 is built,\n",
      "tree # 92 is built,\n",
      "tree # 93 is built,\n",
      "tree # 94 is built,\n",
      "tree # 95 is built,\n",
      "tree # 96 is built,\n",
      "tree # 97 is built,\n",
      "tree # 98 is built,\n",
      "tree # 99 is built,\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.93      0.93      0.93        14\n",
      "         vul       0.86      0.86      0.86         7\n",
      "\n",
      "    accuracy                           0.90        21\n",
      "   macro avg       0.89      0.89      0.89        21\n",
      "weighted avg       0.90      0.90      0.90        21\n",
      "\n",
      "\n",
      "Accuracy of prediction is: 0.9047619047619048\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8  10  12  13  14  15  16  17  18  19\n",
      "  20  23  26  28  29  30  31  33  36  38  39  40  41  42  43  44  45  46\n",
      "  47  49  50  52  54  55  56  57  58  59  60  61  62  63  64  66  67  68\n",
      "  69  70  71  72  74  75  76  77  78  79  80  81  82  84  86  87  89  90\n",
      "  92  93  94  95  96  97  98 100 101 102 103 104] \n",
      "TEST: [ 9 11 21 22 24 25 27 32 34 35 37 48 51 53 65 73 83 85 88 91 99]\n",
      "tree # 0 is built,\n",
      "tree # 1 is built,\n",
      "tree # 2 is built,\n",
      "tree # 3 is built,\n",
      "tree # 4 is built,\n",
      "tree # 5 is built,\n",
      "tree # 6 is built,\n",
      "tree # 7 is built,\n",
      "tree # 8 is built,\n",
      "tree # 9 is built,\n",
      "tree # 10 is built,\n",
      "tree # 11 is built,\n",
      "tree # 12 is built,\n",
      "tree # 13 is built,\n",
      "tree # 14 is built,\n",
      "tree # 15 is built,\n",
      "tree # 16 is built,\n",
      "tree # 17 is built,\n",
      "tree # 18 is built,\n",
      "tree # 19 is built,\n",
      "tree # 20 is built,\n",
      "tree # 21 is built,\n",
      "tree # 22 is built,\n",
      "tree # 23 is built,\n",
      "tree # 24 is built,\n",
      "tree # 25 is built,\n",
      "tree # 26 is built,\n",
      "tree # 27 is built,\n",
      "tree # 28 is built,\n",
      "tree # 29 is built,\n",
      "tree # 30 is built,\n",
      "tree # 31 is built,\n",
      "tree # 32 is built,\n",
      "tree # 33 is built,\n",
      "tree # 34 is built,\n",
      "tree # 35 is built,\n",
      "tree # 36 is built,\n",
      "tree # 37 is built,\n",
      "tree # 38 is built,\n",
      "tree # 39 is built,\n",
      "tree # 40 is built,\n",
      "tree # 41 is built,\n",
      "tree # 42 is built,\n",
      "tree # 43 is built,\n",
      "tree # 44 is built,\n",
      "tree # 45 is built,\n",
      "tree # 46 is built,\n",
      "tree # 47 is built,\n",
      "tree # 48 is built,\n",
      "tree # 49 is built,\n",
      "tree # 50 is built,\n",
      "tree # 51 is built,\n",
      "tree # 52 is built,\n",
      "tree # 53 is built,\n",
      "tree # 54 is built,\n",
      "tree # 55 is built,\n",
      "tree # 56 is built,\n",
      "tree # 57 is built,\n",
      "tree # 58 is built,\n",
      "tree # 59 is built,\n",
      "tree # 60 is built,\n",
      "tree # 61 is built,\n",
      "tree # 62 is built,\n",
      "tree # 63 is built,\n",
      "tree # 64 is built,\n",
      "tree # 65 is built,\n",
      "tree # 66 is built,\n",
      "tree # 67 is built,\n",
      "tree # 68 is built,\n",
      "tree # 69 is built,\n",
      "tree # 70 is built,\n",
      "tree # 71 is built,\n",
      "tree # 72 is built,\n",
      "tree # 73 is built,\n",
      "tree # 74 is built,\n",
      "tree # 75 is built,\n",
      "tree # 76 is built,\n",
      "tree # 77 is built,\n",
      "tree # 78 is built,\n",
      "tree # 79 is built,\n",
      "tree # 80 is built,\n",
      "tree # 81 is built,\n",
      "tree # 82 is built,\n",
      "tree # 83 is built,\n",
      "tree # 84 is built,\n",
      "tree # 85 is built,\n",
      "tree # 86 is built,\n",
      "tree # 87 is built,\n",
      "tree # 88 is built,\n",
      "tree # 89 is built,\n",
      "tree # 90 is built,\n",
      "tree # 91 is built,\n",
      "tree # 92 is built,\n",
      "tree # 93 is built,\n",
      "tree # 94 is built,\n",
      "tree # 95 is built,\n",
      "tree # 96 is built,\n",
      "tree # 97 is built,\n",
      "tree # 98 is built,\n",
      "tree # 99 is built,\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.92      0.92      0.92        13\n",
      "         vul       0.88      0.88      0.88         8\n",
      "\n",
      "    accuracy                           0.90        21\n",
      "   macro avg       0.90      0.90      0.90        21\n",
      "weighted avg       0.90      0.90      0.90        21\n",
      "\n",
      "\n",
      "Accuracy of prediction is: 0.9047619047619048\n",
      "========================================================\n",
      "========================================================\n",
      "TRAIN: [  0   1   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  21  22  24  25  26  27  29  30  32  33  34  35  36  37  38  39  40\n",
      "  41  42  43  44  46  47  48  49  50  51  52  53  54  55  56  57  58  60\n",
      "  61  62  63  65  68  69  70  71  73  74  75  76  78  80  81  82  83  85\n",
      "  87  88  89  90  91  94  95  96  99 100 101 104] \n",
      "TEST: [  7  20  23  28  31  45  59  64  66  67  72  77  79  84  86  92  93  97\n",
      "  98 102 103]\n",
      "tree # 0 is built,\n",
      "tree # 1 is built,\n",
      "tree # 2 is built,\n",
      "tree # 3 is built,\n",
      "tree # 4 is built,\n",
      "tree # 5 is built,\n",
      "tree # 6 is built,\n",
      "tree # 7 is built,\n",
      "tree # 8 is built,\n",
      "tree # 9 is built,\n",
      "tree # 10 is built,\n",
      "tree # 11 is built,\n",
      "tree # 12 is built,\n",
      "tree # 13 is built,\n",
      "tree # 14 is built,\n",
      "tree # 15 is built,\n",
      "tree # 16 is built,\n",
      "tree # 17 is built,\n",
      "tree # 18 is built,\n",
      "tree # 19 is built,\n",
      "tree # 20 is built,\n",
      "tree # 21 is built,\n",
      "tree # 22 is built,\n",
      "tree # 23 is built,\n",
      "tree # 24 is built,\n",
      "tree # 25 is built,\n",
      "tree # 26 is built,\n",
      "tree # 27 is built,\n",
      "tree # 28 is built,\n",
      "tree # 29 is built,\n",
      "tree # 30 is built,\n",
      "tree # 31 is built,\n",
      "tree # 32 is built,\n",
      "tree # 33 is built,\n",
      "tree # 34 is built,\n",
      "tree # 35 is built,\n",
      "tree # 36 is built,\n",
      "tree # 37 is built,\n",
      "tree # 38 is built,\n",
      "tree # 39 is built,\n",
      "tree # 40 is built,\n",
      "tree # 41 is built,\n",
      "tree # 42 is built,\n",
      "tree # 43 is built,\n",
      "tree # 44 is built,\n",
      "tree # 45 is built,\n",
      "tree # 46 is built,\n",
      "tree # 47 is built,\n",
      "tree # 48 is built,\n",
      "tree # 49 is built,\n",
      "tree # 50 is built,\n",
      "tree # 51 is built,\n",
      "tree # 52 is built,\n",
      "tree # 53 is built,\n",
      "tree # 54 is built,\n",
      "tree # 55 is built,\n",
      "tree # 56 is built,\n",
      "tree # 57 is built,\n",
      "tree # 58 is built,\n",
      "tree # 59 is built,\n",
      "tree # 60 is built,\n",
      "tree # 61 is built,\n",
      "tree # 62 is built,\n",
      "tree # 63 is built,\n",
      "tree # 64 is built,\n",
      "tree # 65 is built,\n",
      "tree # 66 is built,\n",
      "tree # 67 is built,\n",
      "tree # 68 is built,\n",
      "tree # 69 is built,\n",
      "tree # 70 is built,\n",
      "tree # 71 is built,\n",
      "tree # 72 is built,\n",
      "tree # 73 is built,\n",
      "tree # 74 is built,\n",
      "tree # 75 is built,\n",
      "tree # 76 is built,\n",
      "tree # 77 is built,\n",
      "tree # 78 is built,\n",
      "tree # 79 is built,\n",
      "tree # 80 is built,\n",
      "tree # 81 is built,\n",
      "tree # 82 is built,\n",
      "tree # 83 is built,\n",
      "tree # 84 is built,\n",
      "tree # 85 is built,\n",
      "tree # 86 is built,\n",
      "tree # 87 is built,\n",
      "tree # 88 is built,\n",
      "tree # 89 is built,\n",
      "tree # 90 is built,\n",
      "tree # 91 is built,\n",
      "tree # 92 is built,\n",
      "tree # 93 is built,\n",
      "tree # 94 is built,\n",
      "tree # 95 is built,\n",
      "tree # 96 is built,\n",
      "tree # 97 is built,\n",
      "tree # 98 is built,\n",
      "tree # 99 is built,\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        safe       0.91      1.00      0.95        10\n",
      "         vul       1.00      0.91      0.95        11\n",
      "\n",
      "    accuracy                           0.95        21\n",
      "   macro avg       0.95      0.95      0.95        21\n",
      "weighted avg       0.96      0.95      0.95        21\n",
      "\n",
      "\n",
      "Accuracy of prediction is: 0.9523809523809523\n",
      "========================================================\n",
      "========================================================\n",
      "*********************************************************\n",
      "\n",
      "Average accuracy: 0.9238095238095237\n"
     ]
    }
   ],
   "source": [
    "# Random Forest \n",
    "\n",
    "# trying to scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "rf = RandomForest()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "labels = list(df['label'])\n",
    "accuracies = list()\n",
    "\n",
    "for train_indices, test_indices in kf.split(df):\n",
    "    print(\"TRAIN:\", train_indices, \"\\nTEST:\", test_indices)\n",
    "    \n",
    "    train_df = df.loc[train_indices, :].copy()\n",
    "    test_df = df.loc[test_indices, :].copy()\n",
    "    \n",
    "    train_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    train_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    \n",
    "    test_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    test_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    test_labels = list(test_df['label'])\n",
    "    test_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    train_labels = list(train_df['label'])\n",
    "    train_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    # scaling:\n",
    "    scaler.fit(train_df)\n",
    "    train_ndarray = scaler.transform(train_df)\n",
    "    test_ndarray = scaler.transform(test_df)\n",
    "\n",
    "    # let's add the labels to the train_df after scalling, the fit function in our Random Forest classifier needs them:\n",
    "    train_df = pd.DataFrame(train_ndarray)\n",
    "    train_df['label'] = train_labels\n",
    "    test_df = pd.DataFrame(test_ndarray)\n",
    "    \n",
    "    rf.fit(train_df, no_trees=100)\n",
    "    predictions = rf.predict(test_df)\n",
    "\n",
    "    y_pred = get_ys_from_prediction(predictions)\n",
    "    print(classification_report(test_labels, y_pred))\n",
    "\n",
    "    #print(test_labels)\n",
    "    #print(y_pred)\n",
    "\n",
    "    ac = metrics.accuracy_score(test_labels, y_pred)\n",
    "    #ac = accuracy(predictions, labels)\n",
    "    accuracies.append(ac)\n",
    "\n",
    "    \n",
    "    print(\"\\nAccuracy of prediction is: {}\".format(ac))\n",
    "    print(\"========================================================\")\n",
    "    print(\"========================================================\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"*********************************************************\")\n",
    "print(\"\\nAverage accuracy: {}\".format(sum(accuracies) / len(accuracies)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN: [  0   1   2   3   4   6  10  11  12  14  15  16  17  18  19  20  21  23\n  24  25  26  27  29  30  31  32  33  34  35  36  38  39  40  41  42  43\n  44  45  46  48  50  51  52  53  54  55  56  58  59  62  63  64  65  66\n  67  68  70  71  73  74  75  78  79  80  81  82  83  84  85  86  87  88\n  89  91  93  95  96  97  98  99 100 101 102 104] \nTEST: [  5   7   8   9  13  22  28  37  47  49  57  60  61  69  72  76  77  90\n  92  94 103]\n              precision    recall  f1-score   support\n\n        safe       0.79      1.00      0.88        11\n         vul       1.00      0.70      0.82        10\n\n    accuracy                           0.86        21\n   macro avg       0.89      0.85      0.85        21\nweighted avg       0.89      0.86      0.85        21\n\n\nAccuracy of prediction is: 0.8571428571428571\n========================================================\n========================================================\nTRAIN: [  1   2   3   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n  21  22  23  24  25  26  27  28  29  32  33  35  36  37  38  39  41  43\n  44  45  47  49  50  51  52  53  56  57  58  60  61  62  63  64  65  66\n  67  68  69  70  71  72  73  74  75  76  77  82  83  84  85  86  88  90\n  91  92  93  94  95  97  98  99 100 102 103 104] \nTEST: [  0   4  20  30  31  34  40  42  46  48  54  55  59  78  79  80  81  87\n  89  96 101]\n              precision    recall  f1-score   support\n\n        safe       0.79      1.00      0.88        11\n         vul       1.00      0.70      0.82        10\n\n    accuracy                           0.86        21\n   macro avg       0.89      0.85      0.85        21\nweighted avg       0.89      0.86      0.85        21\n\n\nAccuracy of prediction is: 0.8571428571428571\n========================================================\n========================================================\nTRAIN: [  0   1   4   5   6   7   8   9  10  11  13  14  15  16  18  19  20  21\n  22  24  26  27  28  29  30  31  32  33  34  37  39  40  41  42  43  45\n  46  47  48  49  50  51  52  53  54  55  57  58  59  60  61  63  64  67\n  68  69  70  71  72  73  75  76  77  78  79  80  81  82  83  84  86  87\n  88  89  90  92  93  94  96  97  98 100 101 103] \nTEST: [  2   3  12  17  23  25  35  36  38  44  56  62  65  66  74  85  91  95\n  99 102 104]\n              precision    recall  f1-score   support\n\n        safe       1.00      1.00      1.00        11\n         vul       1.00      1.00      1.00        10\n\n    accuracy                           1.00        21\n   macro avg       1.00      1.00      1.00        21\nweighted avg       1.00      1.00      1.00        21\n\n\nAccuracy of prediction is: 1.0\n========================================================\n========================================================\nTRAIN: [  0   1   2   3   4   5   6   7   8   9  12  13  14  17  18  20  22  23\n  24  25  27  28  29  30  31  32  33  34  35  36  37  38  40  42  43  44\n  45  46  47  48  49  53  54  55  56  57  59  60  61  62  64  65  66  67\n  69  70  72  74  75  76  77  78  79  80  81  83  85  86  87  88  89  90\n  91  92  93  94  95  96  98  99 101 102 103 104] \nTEST: [ 10  11  15  16  19  21  26  39  41  50  51  52  58  63  68  71  73  82\n  84  97 100]\n              precision    recall  f1-score   support\n\n        safe       0.87      1.00      0.93        13\n         vul       1.00      0.75      0.86         8\n\n    accuracy                           0.90        21\n   macro avg       0.93      0.88      0.89        21\nweighted avg       0.92      0.90      0.90        21\n\n\nAccuracy of prediction is: 0.9047619047619048\n========================================================\n========================================================\nTRAIN: [  0   2   3   4   5   7   8   9  10  11  12  13  15  16  17  19  20  21\n  22  23  25  26  28  30  31  34  35  36  37  38  39  40  41  42  44  46\n  47  48  49  50  51  52  54  55  56  57  58  59  60  61  62  63  65  66\n  68  69  71  72  73  74  76  77  78  79  80  81  82  84  85  87  89  90\n  91  92  94  95  96  97  99 100 101 102 103 104] \nTEST: [ 1  6 14 18 24 27 29 32 33 43 45 53 64 67 70 75 83 86 88 93 98]\n              precision    recall  f1-score   support\n\n        safe       0.77      1.00      0.87        10\n         vul       1.00      0.73      0.84        11\n\n    accuracy                           0.86        21\n   macro avg       0.88      0.86      0.86        21\nweighted avg       0.89      0.86      0.86        21\n\n\nAccuracy of prediction is: 0.8571428571428571\n========================================================\n========================================================\n*********************************************************\n\nAverage accuracy: 0.8952380952380953\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# trying to scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "labels = list(df['label'])\n",
    "accuracies = list()\n",
    "\n",
    "for train_indices, test_indices in kf.split(df):\n",
    "    print(\"TRAIN:\", train_indices, \"\\nTEST:\", test_indices)\n",
    "    \n",
    "    train_df = df.loc[train_indices, :].copy()\n",
    "    test_df = df.loc[test_indices, :].copy()\n",
    "    \n",
    "    train_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    train_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    \n",
    "    test_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    test_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    train_labels = list(train_df['label'])\n",
    "    train_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    test_labels = list(test_df['label'])\n",
    "    test_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    # scaling:\n",
    "    scaler.fit(train_df)\n",
    "    train_df = scaler.transform(train_df)\n",
    "    test_df = scaler.transform(test_df)\n",
    "\n",
    "    # training\n",
    "    logreg = LogisticRegression() \n",
    "    logreg.fit(train_df, train_labels)\n",
    "\n",
    "    # prediction\n",
    "    y_pred=logreg.predict(test_df)\n",
    "    #cnf_matrix = metrics.confusion_matrix(test_labels, y_pred)\n",
    "    \n",
    "    # measuring performance\n",
    "    print(classification_report(test_labels, y_pred))  \n",
    "    ac = metrics.accuracy_score(test_labels, y_pred)\n",
    "    accuracies.append(ac)\n",
    "    \n",
    "    print(\"\\nAccuracy of prediction is: {}\".format(ac))\n",
    "    print(\"========================================================\")\n",
    "    print(\"========================================================\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"*********************************************************\")\n",
    "print(\"\\nAverage accuracy: {}\".format(sum(accuracies) / len(accuracies)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN: [  1   5   8   9  10  11  12  13  15  16  17  19  21  22  23  24  25  26\n  27  28  29  30  31  32  33  35  36  37  38  40  41  42  43  44  45  46\n  47  48  50  54  55  57  58  59  61  62  63  64  66  67  69  70  72  73\n  74  75  76  77  78  79  81  82  83  84  85  86  87  88  89  90  91  92\n  93  94  95  96  97  98  99 100 101 102 103 104] \nTEST: [ 0  2  3  4  6  7 14 18 20 34 39 49 51 52 53 56 60 65 68 71 80]\n              precision    recall  f1-score   support\n\n        safe       0.65      1.00      0.79        11\n         vul       1.00      0.40      0.57        10\n\n    accuracy                           0.71        21\n   macro avg       0.82      0.70      0.68        21\nweighted avg       0.82      0.71      0.68        21\n\n\nAccuracy of prediction is: 0.7142857142857143\n========================================================\n========================================================\nTRAIN: [  0   1   2   3   4   5   6   7   8   9  10  12  13  14  15  16  17  18\n  19  20  21  22  23  25  26  28  29  30  31  33  34  35  36  37  38  39\n  41  45  46  48  49  50  51  52  53  54  55  56  57  58  59  60  61  63\n  64  65  66  67  68  70  71  72  73  75  77  78  79  80  81  82  83  84\n  85  89  90  92  93  95  98 100 101 102 103 104] \nTEST: [11 24 27 32 40 42 43 44 47 62 69 74 76 86 87 88 91 94 96 97 99]\n              precision    recall  f1-score   support\n\n        safe       0.82      1.00      0.90         9\n         vul       1.00      0.83      0.91        12\n\n    accuracy                           0.90        21\n   macro avg       0.91      0.92      0.90        21\nweighted avg       0.92      0.90      0.91        21\n\n\nAccuracy of prediction is: 0.9047619047619048\n========================================================\n========================================================\nTRAIN: [  0   2   3   4   5   6   7   8  11  12  13  14  15  16  18  19  20  21\n  24  25  27  29  30  31  32  34  35  36  37  38  39  40  41  42  43  44\n  46  47  48  49  50  51  52  53  55  56  60  62  63  64  65  66  67  68\n  69  70  71  73  74  75  76  77  80  82  83  84  85  86  87  88  89  91\n  92  94  95  96  97  98  99 100 101 102 103 104] \nTEST: [ 1  9 10 17 22 23 26 28 33 45 54 57 58 59 61 72 78 79 81 90 93]\n              precision    recall  f1-score   support\n\n        safe       0.67      0.91      0.77        11\n         vul       0.83      0.50      0.62        10\n\n    accuracy                           0.71        21\n   macro avg       0.75      0.70      0.70        21\nweighted avg       0.75      0.71      0.70        21\n\n\nAccuracy of prediction is: 0.7142857142857143\n========================================================\n========================================================\nTRAIN: [  0   1   2   3   4   6   7   9  10  11  12  13  14  15  16  17  18  19\n  20  22  23  24  26  27  28  31  32  33  34  35  37  39  40  41  42  43\n  44  45  46  47  48  49  50  51  52  53  54  56  57  58  59  60  61  62\n  63  65  68  69  71  72  74  75  76  78  79  80  81  82  83  86  87  88\n  89  90  91  93  94  96  97  98  99 100 101 103] \nTEST: [  5   8  21  25  29  30  36  38  55  64  66  67  70  73  77  84  85  92\n  95 102 104]\n              precision    recall  f1-score   support\n\n        safe       0.92      0.92      0.92        13\n         vul       0.88      0.88      0.88         8\n\n    accuracy                           0.90        21\n   macro avg       0.90      0.90      0.90        21\nweighted avg       0.90      0.90      0.90        21\n\n\nAccuracy of prediction is: 0.9047619047619048\n========================================================\n========================================================\nTRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  14  17  18  20  21  22\n  23  24  25  26  27  28  29  30  32  33  34  36  38  39  40  42  43  44\n  45  47  49  51  52  53  54  55  56  57  58  59  60  61  62  64  65  66\n  67  68  69  70  71  72  73  74  76  77  78  79  80  81  84  85  86  87\n  88  90  91  92  93  94  95  96  97  99 102 104] \nTEST: [ 12  13  15  16  19  31  35  37  41  46  48  50  63  75  82  83  89  98\n 100 101 103]\n              precision    recall  f1-score   support\n\n        safe       0.80      1.00      0.89        12\n         vul       1.00      0.67      0.80         9\n\n    accuracy                           0.86        21\n   macro avg       0.90      0.83      0.84        21\nweighted avg       0.89      0.86      0.85        21\n\n\nAccuracy of prediction is: 0.8571428571428571\n========================================================\n========================================================\n*********************************************************\n\nAverage accuracy: 0.819047619047619\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "# trying to scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "labels = list(df['label'])\n",
    "accuracies = list()\n",
    "\n",
    "\n",
    "for train_indices, test_indices in kf.split(df):\n",
    "    print(\"TRAIN:\", train_indices, \"\\nTEST:\", test_indices)\n",
    "    \n",
    "    train_df = df.loc[train_indices, :].copy()\n",
    "    test_df = df.loc[test_indices, :].copy()\n",
    "    \n",
    "    train_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    train_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    \n",
    "    test_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    test_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    train_labels = list(train_df['label'])\n",
    "    train_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    test_labels = list(test_df['label'])\n",
    "    test_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    # scaling:\n",
    "    scaler.fit(train_df)\n",
    "    train_df = scaler.transform(train_df)\n",
    "    test_df = scaler.transform(test_df)\n",
    "\n",
    "    classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "    # training\n",
    "    classifier.fit(train_df, train_labels)\n",
    "\n",
    "    # prediction\n",
    "    y_pred = classifier.predict(test_df)\n",
    "    #print(confusion_matrix(test_labels, y_pred))\n",
    "    print(classification_report(test_labels, y_pred))    \n",
    "\n",
    "    # measuring performance\n",
    "    ac = metrics.accuracy_score(test_labels, y_pred)\n",
    "    accuracies.append(ac)\n",
    "    \n",
    "    print(\"\\nAccuracy of prediction is: {}\".format(ac))\n",
    "    print(\"========================================================\")\n",
    "    print(\"========================================================\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"*********************************************************\")\n",
    "print(\"\\nAverage accuracy: {}\".format(sum(accuracies) / len(accuracies)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN: [  0   1   2   4   5   6   7   8   9  12  13  14  15  16  17  18  19  20\n  21  22  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38  40\n  42  43  46  47  48  49  50  53  54  57  58  59  60  61  62  63  64  65\n  67  69  70  71  72  73  74  75  76  77  79  81  83  84  85  86  87  88\n  89  90  92  93  94  95  96  97  99 100 101 102] \nTEST: [  3  10  11  29  39  41  44  45  51  52  55  56  66  68  78  80  82  91\n  98 103 104]\n********\n['safe' 'safe' 'safe' 'safe' 'safe' 'vul' 'safe' 'safe' 'safe' 'safe'\n 'vul' 'vul' 'vul' 'vul' 'safe' 'safe' 'vul' 'vul' 'vul' 'vul' 'vul']\n********\n              precision    recall  f1-score   support\n\n        safe       0.91      1.00      0.95        10\n         vul       1.00      0.91      0.95        11\n\n    accuracy                           0.95        21\n   macro avg       0.95      0.95      0.95        21\nweighted avg       0.96      0.95      0.95        21\n\n\nAccuracy of prediction is: 0.9523809523809523\n========================================================\n========================================================\nTRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  15  16  18  19\n  20  21  23  25  26  27  28  29  31  32  34  35  36  38  39  40  41  42\n  44  45  48  49  50  51  52  53  54  55  56  57  58  59  60  62  63  66\n  67  68  69  71  72  73  74  76  78  79  80  81  82  83  84  85  87  88\n  89  90  91  92  97  98  99 100 101 102 103 104] \nTEST: [ 6 17 22 24 30 33 37 43 46 47 61 64 65 70 75 77 86 93 94 95 96]\n********\n['vul' 'vul' 'safe' 'vul' 'vul' 'safe' 'vul' 'safe' 'safe' 'vul' 'safe'\n 'safe' 'safe' 'vul' 'safe' 'safe' 'safe' 'safe' 'safe' 'vul' 'safe']\n********\n              precision    recall  f1-score   support\n\n        safe       0.85      1.00      0.92        11\n         vul       1.00      0.80      0.89        10\n\n    accuracy                           0.90        21\n   macro avg       0.92      0.90      0.90        21\nweighted avg       0.92      0.90      0.90        21\n\n\nAccuracy of prediction is: 0.9047619047619048\n========================================================\n========================================================\nTRAIN: [  1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  21  22\n  23  24  25  27  29  30  31  32  33  34  36  37  38  39  40  41  42  43\n  44  45  46  47  48  49  51  52  53  55  56  57  61  62  63  64  65  66\n  67  68  70  71  72  74  75  76  77  78  79  80  82  85  86  88  89  90\n  91  93  94  95  96  97  98  99 100 101 103 104] \nTEST: [  0   8  18  19  20  26  28  35  50  54  58  59  60  69  73  81  83  84\n  87  92 102]\n********\n['safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe'\n 'vul' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'safe' 'vul' 'safe'\n 'safe']\n********\n              precision    recall  f1-score   support\n\n        safe       0.74      1.00      0.85        14\n         vul       1.00      0.29      0.44         7\n\n    accuracy                           0.76        21\n   macro avg       0.87      0.64      0.65        21\nweighted avg       0.82      0.76      0.71        21\n\n\nAccuracy of prediction is: 0.7619047619047619\n========================================================\n========================================================\nTRAIN: [  0   1   2   3   6   8  10  11  14  16  17  18  19  20  22  23  24  25\n  26  27  28  29  30  32  33  34  35  37  38  39  40  41  43  44  45  46\n  47  48  50  51  52  53  54  55  56  57  58  59  60  61  62  64  65  66\n  67  68  69  70  72  73  75  76  77  78  80  81  82  83  84  86  87  89\n  91  92  93  94  95  96  97  98 100 102 103 104] \nTEST: [  4   5   7   9  12  13  15  21  31  36  42  49  63  71  74  79  85  88\n  90  99 101]\n********\n['safe' 'safe' 'vul' 'safe' 'safe' 'safe' 'safe' 'safe' 'vul' 'vul' 'vul'\n 'vul' 'safe' 'safe' 'vul' 'safe' 'safe' 'vul' 'vul' 'vul' 'safe']\n********\n              precision    recall  f1-score   support\n\n        safe       0.92      0.79      0.85        14\n         vul       0.67      0.86      0.75         7\n\n    accuracy                           0.81        21\n   macro avg       0.79      0.82      0.80        21\nweighted avg       0.83      0.81      0.81        21\n\n\nAccuracy of prediction is: 0.8095238095238095\n========================================================\n========================================================\nTRAIN: [  0   3   4   5   6   7   8   9  10  11  12  13  15  17  18  19  20  21\n  22  24  26  28  29  30  31  33  35  36  37  39  41  42  43  44  45  46\n  47  49  50  51  52  54  55  56  58  59  60  61  63  64  65  66  68  69\n  70  71  73  74  75  77  78  79  80  81  82  83  84  85  86  87  88  90\n  91  92  93  94  95  96  98  99 101 102 103 104] \nTEST: [  1   2  14  16  23  25  27  32  34  38  40  48  53  57  62  67  72  76\n  89  97 100]\n********\n['vul' 'safe' 'vul' 'vul' 'vul' 'safe' 'safe' 'vul' 'safe' 'safe' 'safe'\n 'safe' 'vul' 'vul' 'vul' 'vul' 'vul' 'safe' 'vul' 'vul' 'safe']\n********\n              precision    recall  f1-score   support\n\n        safe       0.78      1.00      0.88         7\n         vul       1.00      0.86      0.92        14\n\n    accuracy                           0.90        21\n   macro avg       0.89      0.93      0.90        21\nweighted avg       0.93      0.90      0.91        21\n\n\nAccuracy of prediction is: 0.9047619047619048\n========================================================\n========================================================\n*********************************************************\n\nAverage accuracy: 0.8666666666666668\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "# trying to scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "labels = list(df['label'])\n",
    "accuracies = list()\n",
    "\n",
    "\n",
    "for train_indices, test_indices in kf.split(df):\n",
    "    print(\"TRAIN:\", train_indices, \"\\nTEST:\", test_indices)\n",
    "    \n",
    "    train_df = df.loc[train_indices, :].copy()\n",
    "    test_df = df.loc[test_indices, :].copy()\n",
    "    \n",
    "    train_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    train_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    \n",
    "    test_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    test_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    train_labels = list(train_df['label'])\n",
    "    train_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    test_labels = list(test_df['label'])\n",
    "    test_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    # scaling:\n",
    "    scaler.fit(train_df)\n",
    "    #train_df = scaler.transform(train_df)\n",
    "    #test_df = scaler.transform(test_df)\n",
    "\n",
    "    # Gaussian Naive Bayes classier:\n",
    "    gnb = GaussianNB()\n",
    "\n",
    "    # training\n",
    "    gnb.fit(train_df, train_labels)\n",
    "\n",
    "    # prediction\n",
    "    y_pred = gnb.predict(test_df)\n",
    "    print(\"********\")\n",
    "    print(y_pred)\n",
    "    print(\"********\")\n",
    "    print(classification_report(test_labels, y_pred))    \n",
    "\n",
    "    # measuring performance\n",
    "    ac = metrics.accuracy_score(test_labels, y_pred)\n",
    "    accuracies.append(ac)\n",
    "    \n",
    "    print(\"\\nAccuracy of prediction is: {}\".format(ac))\n",
    "    print(\"========================================================\")\n",
    "    print(\"========================================================\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"*********************************************************\")\n",
    "print(\"\\nAverage accuracy: {}\".format(sum(accuracies) / len(accuracies)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN: [  0   1   2   4   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n  21  22  23  24  25  26  27  28  29  30  33  34  36  37  38  39  40  41\n  42  44  46  47  49  50  52  54  55  56  58  59  60  61  62  63  64  66\n  68  69  70  72  73  75  76  77  78  80  81  82  84  86  87  88  89  90\n  91  92  93  94  95  96  97 100 101 102 103 104] \nTEST: [ 3  5 20 31 32 35 43 45 48 51 53 57 65 67 71 74 79 83 85 98 99]\n              precision    recall  f1-score   support\n\n        safe       0.77      0.91      0.83        11\n         vul       0.88      0.70      0.78        10\n\n    accuracy                           0.81        21\n   macro avg       0.82      0.80      0.81        21\nweighted avg       0.82      0.81      0.81        21\n\n\nAccuracy of prediction is: 0.8095238095238095\n========================================================\n========================================================\nTRAIN: [  0   1   2   3   4   5   6   8   9  10  13  14  15  16  17  18  20  21\n  23  26  27  28  29  31  32  33  34  35  36  37  38  39  40  42  43  44\n  45  46  47  48  49  51  53  54  55  56  57  58  59  60  63  64  65  66\n  67  68  71  73  74  75  76  78  79  80  81  83  84  85  86  87  88  89\n  90  91  92  93  94  95  96  97  98  99 101 104] \nTEST: [  7  11  12  19  22  24  25  30  41  50  52  61  62  69  70  72  77  82\n 100 102 103]\n              precision    recall  f1-score   support\n\n        safe       0.86      1.00      0.92        12\n         vul       1.00      0.78      0.88         9\n\n    accuracy                           0.90        21\n   macro avg       0.93      0.89      0.90        21\nweighted avg       0.92      0.90      0.90        21\n\n\nAccuracy of prediction is: 0.9047619047619048\n========================================================\n========================================================\nTRAIN: [  0   2   3   5   6   7   8   9  10  11  12  13  15  16  17  19  20  21\n  22  23  24  25  26  27  28  29  30  31  32  33  35  36  37  40  41  42\n  43  45  48  50  51  52  53  56  57  58  59  60  61  62  63  64  65  66\n  67  68  69  70  71  72  73  74  76  77  78  79  81  82  83  84  85  87\n  88  92  93  94  95  96  97  98  99 100 102 103] \nTEST: [  1   4  14  18  34  38  39  44  46  47  49  54  55  75  80  86  89  90\n  91 101 104]\n              precision    recall  f1-score   support\n\n        safe       1.00      0.85      0.92        13\n         vul       0.80      1.00      0.89         8\n\n    accuracy                           0.90        21\n   macro avg       0.90      0.92      0.90        21\nweighted avg       0.92      0.90      0.91        21\n\n\nAccuracy of prediction is: 0.9047619047619048\n========================================================\n========================================================\nTRAIN: [  1   2   3   4   5   7   9  11  12  14  16  18  19  20  21  22  23  24\n  25  26  27  30  31  32  34  35  38  39  41  43  44  45  46  47  48  49\n  50  51  52  53  54  55  56  57  58  59  60  61  62  64  65  67  69  70\n  71  72  74  75  76  77  78  79  80  81  82  83  85  86  87  88  89  90\n  91  93  94  95  96  98  99 100 101 102 103 104] \nTEST: [ 0  6  8 10 13 15 17 28 29 33 36 37 40 42 63 66 68 73 84 92 97]\n              precision    recall  f1-score   support\n\n        safe       0.92      1.00      0.96        12\n         vul       1.00      0.89      0.94         9\n\n    accuracy                           0.95        21\n   macro avg       0.96      0.94      0.95        21\nweighted avg       0.96      0.95      0.95        21\n\n\nAccuracy of prediction is: 0.9523809523809523\n========================================================\n========================================================\nTRAIN: [  0   1   3   4   5   6   7   8  10  11  12  13  14  15  17  18  19  20\n  22  24  25  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42\n  43  44  45  46  47  48  49  50  51  52  53  54  55  57  61  62  63  65\n  66  67  68  69  70  71  72  73  74  75  77  79  80  82  83  84  85  86\n  89  90  91  92  97  98  99 100 101 102 103 104] \nTEST: [ 2  9 16 21 23 26 27 56 58 59 60 64 76 78 81 87 88 93 94 95 96]\n              precision    recall  f1-score   support\n\n        safe       0.50      1.00      0.67         8\n         vul       1.00      0.38      0.56        13\n\n    accuracy                           0.62        21\n   macro avg       0.75      0.69      0.61        21\nweighted avg       0.81      0.62      0.60        21\n\n\nAccuracy of prediction is: 0.6190476190476191\n========================================================\n========================================================\n*********************************************************\n\nAverage accuracy: 0.838095238095238\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier\n",
    "\n",
    "# trying to scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "labels = list(df['label'])\n",
    "accuracies = list()\n",
    "\n",
    "\n",
    "for train_indices, test_indices in kf.split(df):\n",
    "    print(\"TRAIN:\", train_indices, \"\\nTEST:\", test_indices)\n",
    "    \n",
    "    train_df = df.loc[train_indices, :].copy()\n",
    "    test_df = df.loc[test_indices, :].copy()\n",
    "    \n",
    "    train_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    train_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    \n",
    "    test_df.drop(columns=['tx_hash'], inplace=True)\n",
    "    test_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    train_labels = list(train_df['label'])\n",
    "    train_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    test_labels = list(test_df['label'])\n",
    "    test_df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    # scaling:\n",
    "    scaler.fit(train_df)\n",
    "    train_df = scaler.transform(train_df)\n",
    "    test_df = scaler.transform(test_df)\n",
    "\n",
    "    # SVM classier:\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "\n",
    "    # training\n",
    "    clf.fit(train_df, train_labels)\n",
    "\n",
    "    # prediction\n",
    "    y_pred = clf.predict(test_df)\n",
    "    print(classification_report(test_labels, y_pred))    \n",
    "\n",
    "    # measuring performance\n",
    "    ac = metrics.accuracy_score(test_labels, y_pred)\n",
    "    accuracies.append(ac)\n",
    "    \n",
    "    print(\"\\nAccuracy of prediction is: {}\".format(ac))\n",
    "    print(\"========================================================\")\n",
    "    print(\"========================================================\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"*********************************************************\")\n",
    "print(\"\\nAverage accuracy: {}\".format(sum(accuracies) / len(accuracies)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}